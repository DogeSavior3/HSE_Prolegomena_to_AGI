= Лекция 6.

Самый важный тег для нас -- сложность. Если мы говорим о сильном искусственном интеллекте, то мы должны говорить о сложности.

Понятие сложности в математике может определяться по-разному.

Существует три центральных подхода:

1) сложно то, что сложно сгенерировать (Колмогоровская сложность). Если для данного массива данных мы должны построить такую машину Тьюринга, что ... -- определение сложности. Практически слабо применимо. 

2) Сложно то, что сложно предсказать. (Предсказательная сложность). Базовая идея, что если мы не можем предсказать поведение системы, то она сложна. 

3) Сложно то, что сложно выучить. (Размерность Вафника-Червоненкиса, $V C$). 

Эти все рассуждения о сложности не являются чисто теоретически. Золотое правило интеллекта: сложность обрабатываемых данных должна равняться сложности системы, которая их обрабатывает. Для того, чтобы оно работало, необходимо хоть как-то определить сложность.

#image("lecture_6_pics/picture1.png", width: 60%)

TODO: сделать другой график

Если мы продолжаем увеличивать сложность системы, то мы получаем 

*рисунок 2*


#pagebreak()

Прежде чем переходить к предсказательная сложности, стоит поговорить про информацию и ............

Шэновская информация. Давайте рассмотрим два вероятностных события

Событие А: Профессор вошел в кабинет.

Событие Б: Профессор убил студента.

Вероятность второго события существенно ниже. После этого Шэнон сказал:

1) $I = f(1/p)$

Где $I$ - информация, $p-$ вероятность.

2) Пусть есть два независимых события, тогда $I = I_1 + I_2, f(1/p_1 1/p_2) = f(1/p_1)f(1/p_2) $

Функция логарифма удовлетворяет такому свойству. С другой стороны можно доказать, что единственная функция, которая удовлетворяет такому свойству -- логарифм.

Тогда $I = ln 1/p = - ln p$

Предположим, что у нас есть система, которая может находиться в $N$ состояниях. И в этих состояниях она находится с вероятностями $p_1, dots, p_n$. Тогда энтропия по Шэнону есть средняя информация, которую мы знаем о такой системе.

$ H = < I > = sum^N_(i = 1) p_i I_i = - sum^N_(i = 1)p_i ln p_i $

Свойства:

Предположим, что мы знаем о системе все, тогда мы можем утверждать, тогда $p_i = 1, $ остальные равны нулю. Если мы все знаем о системе, то ее информационная энтропия равна нулю. 

В обратной ситуации, когда мы не знаем ничего о системе, то $p_i$ равновероятны. $p_i = 1/N => H = ln N => ln N = max H$

Информационная энтропия -- мера неопределенности системы.

Как генерировать хаотические ряды? $X_(n+1) = 1 - lambda X_n^2$

Введенная таким образом Шэновская энтропия является аддитивной в силу выполнения свойства, более того, можно показать, что она является единственной аддитивной энтропией. Между тем в сложных системах обычно имеет место так называемые не аддитивные энтропии, для которых свойство два не выполняется по банальной причине: считается, что не существует двух подлинно информационно изолированной системы. Поэтому в сложных системах целесообразно применять не аддитивные энтропии, то есть энтропии, для которых неверно $I = I_1 + I_2$ 
#pagebreak()
Этих энтропий бывает много, но основые: энтропия Реньи(renyi) 

$ H_q = 1/(1 - q) ln sum^n_(i = 1) p_i^q $ 

Энтропия Цависа(Tsalis), и так далее..

Открытая проблема, к какой сложной сети какую энтропию применять.

Общее методическое замечание: Сложные системы делятся на классы универсальности.  

Предсказательная сложность. Предположим, что у нас идет некоторый поток данных в неком времени, не обязательно метрологическом, и мы можем выделить в нем некоторый момент времени $t = 0$ и два момента: прошлое и будущее. Тогда данные, относящиеся к прошлому мы будет обозначать как $X_(p a s t),$ а к будущему $X_(f u t u r e),$ ... ...$P(X_(p a s t)), P(X_(f u t u r e))$, тогда взаимной информацией между будущем и прошлым будет 
$ I(X_(f u t u r e), X_(p a s t)) = < log_2 (P(X_(f u t u r e) | X_(p a s t)))/P(X_(f u t u r e)) > $

Где усреднение производится по совместному распределению $X_(f u t u r e), X_(p a s t)$.

Эта величина носит название Девергенция Кульбаха Лейблера (Kulback Leibler). Вот математическая мера того, насколько нам сложно предсказать будущее по прошлому. Если мы умножим числитель и знаменатель на $P(X_(p a s t))$, то получим

$ - < log_2 P(X_(p a s t)) > - < log_2 P(X_(f u t u r e))> = -[-<log_2 P(X_(f u t u r e), X_(p a s t))> ]  = $

$ = H(T') + H(T) - H(T + T') $

..........


T = для отрезка future, T' длина отрезка past

Замечание 1:

Выражение, введенное для данной информацией является симметричным, тогда равносильными являются задачи как предсказания, так и подсказания, когда мы прошлое определяем по будущему.

Энтропия состоит из двух состовляющих:

#pagebreak()

первое -- экстенсивная энтропия, и связана с течением времени. То есть, любая система подчиняется второму началу термодинамики. В этом смысле энтропия растет всегда, причем растет пропорционально протекшему времени. Это имеет место абсолютно для любой системы. И это в общем то неинтересно. 

$ H(T) = H_0 dot T + underbrace(H_S (T), o(T))  $

второе -- Суб-экстенсивная энтропия, которая ведет себя как $o(T)$ и определяет поведение системы, которая отличает ее от всех остальных

Мы хотим показать, что разные системы дают нам принципиально разные функции Суб-экстенсивной энтропии как функции времени. Тип этой функции и дает нам класс сложности соответствующей системы. Более того, типов этих функций существует не так уж и много. И это дает нам некоторый первый подход к классификации сложных систем. Пока же нам достаточно отметить, что экстенсивые состовляющие энтропии сокращаются. Соответственно, совместная информация определяется только Суб-экстенсивной энтропией. 

$ H_s (T') + H_s (T) - H_s (T + T') $

Предположим, что мы знаем все прошлое, тогда взаимная информация в пределе 

$ lim_(T' -> + infinity) I(T | T') = H_S (T) = I_(p r e d) (T) $

.........

Здесь можно сказать, что, когда мы говорим о такой хорошей, фундоментальной величине, она должна удовлетворять двум, противоречащим .....

1) она следует из базовых принципов

2) ее можно наблюдать в эксперименте

Метрика хорошо измеряется.
 
