= Лекция 5. Метод Прюснера, Mind-Brain problem.

Метод Прюснера (Pruessner) (self-organized criticality) заключается в том, что при специально выбранных координатах различные выборки, порожденные одним и тем же распределением, дают один и тот же вид распределения. Графически на экране монитора эти зависимости совпадают друг с другом, и мы получаем явление, которое в статистике называется Collapse данных. Соответственно, этот метод выходят далеко за пределы анализа степенных распределений, но Прюснер рассматривал конкретно Collapse данных степенных распределений.

Более того, Прюснер работал с распределениями вида 

$ P(x) = cases(
  .. ", " x < x_min,
  C dot X^(- alpha) g(x "/" x_c)", " x >= x_min,
) $

где С -- константа нормализации, $x_min$ и $alpha -$ нижнее отсечение и параметр степенного распределения,  $x_c -$ характерный размер элементов выборки (максимальное значение элемента в выборке), $g(x "/" x_c) -$ функция горба (hunch) и присутствует в большинстве реальных степенных распределений. Если вы возьмете реально степенное распределение и запишите его в двойном логарифмическом масштабе, вы получите что-то вроде:

#image("lecture_5_pics/image1.jpeg", width: 60%)

Мы получаем характерный горб, математически его существование вызвано наличием функции $g(x "/" x_c)$, а физически его существование связано с тем, что степенное распределение все-таки предполагает, что у нас бесконечность возможных значений, и невероятные события все-таки существуют. Так как выборка конечно, мы этот бесконечный интервал ужимаем до $x_min, x_max.$

Метод Прюснера лучше прошлых методов, так как может работать с горбами. 
#pagebreak() 

Метод Прюснера базируется на двух китах:

1-ый кит) Экспоненциальный биннинг - когда мы говорим о биннинге распределений данных, мы привыкли к равномерному биннингу, все бины имеют одинаковую длину. Для степенных распределений это не самая лучшая идея, для них лучше применять бины не одинаковой длины, а именно бины, длина которых растет экспоненциально с увеличением значения $X$.

Это достаточно понятно, потому что у степенного распределения большая часть данных будет сконентрирована в окресности максимума, но какой-то объем данных, в силу того, что хвост у него тяжелый, будет сконцентрирован дальше. 

#image("lecture_5_pics/gist1.jpeg", width: 70%)

2 кит) Collapse данных. Допустим мы угадали $х_max, alpha$ и $x_c$, то для различных выборок, порожденных одним и тем же распределением, наши зависимости в координатах $p' = P(X) dot x^(-alpha), x slash x_c = x'$ будут иметь один и тот же вид.

$ p' = C dot g(x') $

Прюснер работал с модельными данными, то есть с выборками, порожденными моделями самоорганизованных критичных систем (самыми сложными из сложных систем). В реальных ситуациях, когда у нас есть всего одна выборка, то, если она достаточно большая, мы берем подвыборки из нее (случайно выбранные, 95%, 90%, 80%), и на этих подвыборках моделируем ситуацию с многими выборками, распределениями. При этом надо ясно понимать, что для каждой из этих выбранных выборок $x_c$ будет разным, что обеспечивает нам Collapse данных.

#pagebreak()

Мы берем разные выборки (экспоненциально отбиннингованные), и оцениваем (одним из предыдущих методов) параметр $alpha$. Мы будем обозначать эту оценку $alpha'$, она является очень примерной. Поэтому мы строим наши зависимости в наших координатах $p', x'$. Если нулевая гипотеза о том, что мы действительно имеем дело со степенным распределением верна, то мы получаем следующую картинку: 

#image("lecture_5_pics/image2.jpeg", width: 30%)

У нас есть участок ниже $x_min$ с шумом, у каждой выборки он свой, у нас есть наклонный участок прямой, который отвечает $C dot x^(- alpha)$, и у нас есть характерный изгиб, который Плюснер назвал Landmark. В силу того, что выборки у нас разные, мы получаем разные положения всех трех участков (они сдвинуты друг на друга). С другой стороны, по-скольку $alpha'$ получаена с помощью какой-то грубой оценки, то мы имеем дело с наклонным участком прямой, но то, что этот участок есть, является первый признак того, что распределение, все же, степенное. 

Третий шаг: мы должны оценить истинное значение $alpha$ и $x_c$. Если нам это удастся, то вместо множества графиков, как на предыдущем рисунке, мы получим один график характерного вида, иными словами все графики коллапсируют в один.

#image("lecture_5_pics/image3.jpeg", width: 60%)

В этом графике будет участок меньше $x_min,$ далее горизонтальный участок прямой, указывающий на то, что расрпеделение является степенным, и характерный landmark, отвечающий функции горба $g$. Для осуществления коллапса, перехода от верхнему графику к нижниму, нужно совершить две операции: повернуть график таким образом, чтобы график стал горизонтальным, тем самым получая истинный $alpha, $ и выбрать характерный масштаб таким образом, чтобы все горбы совпали друг с другом (они действительно совпадут). 

Это можно делать вручную, но Прюснер рекомендует использовать МНК, где в качестве данных выступают положения максимумов горбов и значения в этих максимумах. 

Достоинства метода: 

Метод работает с реалистичными степенными распределениями, включающими функцию $g$.

Метод позволяет оценить не просто параметр такого степенного распределения ($x_min, alpha, x_c$), но и проверить нулевую гипотезу о том, что распределение действительно является степенным. Появление этого горизонтального участка прямой является критерием проверки. 

Метод позволяет провести goodness-of-fit test, что дорогого стоит.

Недостатки:

Поскольку он базируется на разных подвыборках, он требует весьма больших выборок, что не всегда возможно в реальных задачах. 

Замечание: теория работы со степенными распределениями является развивающийся областью статистики, и в общем-то задача проверки распределения на степенность является открытой задачей. При практическом использовании целесообразно использовать несколько методов проверки распределения на степенность и делать выводы о степенности, если все три метода дадут положительный ответ с более или менее одинаковыми значениями $x_min, x_max, alpha.$

#pagebreak()

Конетком и Когнитом -- это понятия, введенные в теорию сильного интеллекта академиком Константином Анохиным. 

Конетком -- совокупность нейронов головного мозга человека вместе со совокупностью их аксонно дендридных связей, носят название конеткома (не трудно догадаться, что это некая сложная сеть со всеми особенностями, присущими сложным сетям, с которыми мы уже знакомы)


_Аксонно дендридные связи -- каждый нейрон человека состоит из тела (сомо) и длинного хвоста (аксона), который ведет к другим нейронам. сигналы в головном мозге передаются от одного нейрона к другому через такие аксоно дендритные связи. между ними есть синноптическая щель, которая заполнена нейромедиаторами, сила связи между двумя нейронами определяется концентрацией нейромедиаторов в этой щели. Радость -- повышение нейромедиаторов в щелях._

_Реальное обучение в реальном головном мозге это не изменение концентрации нейромедиаторов, это изменение самой структуры. Все эмоции -- отмирание нейронов и построение новых связей._

Когнитом -- мы можем в том или ином смысле исследовать конетком, но внутреннему наблюдению нам доступны только ментальные состояния человека. Эти состояния также образуют сложную сеть, которая носит название когнитома, и уже принадлежит области психического (не материального). 

Здесь мы сталкиваемся с вечной проблемой нейро-физиологии, которая носит название Mind-Brain problem. Проблема заключается в том, что взаимодействие вполне реальных вещей не вполне понятным нам образом порождает психические явления, то есть явления, относящиеся к сфере духовного. 

Скажем одно, гипотеза Анохина заключается в том, что когнитом представляет собой совокупность когитов, то есть, некоторых временно возникающих совокупностей нейронов головного мозга. Ансамблей нейронов. С математической точки зрения это приводит к понятию сложной гипер-сети, то есть, сложной сети (графа) вершинами которой являются другие сложные сети. ее верхний уровень - когнитом (ментальные состояния), ее самый нижний уровень -- конетком (просто связи между нейронами), но есть промежуточные уровни, которых от 1 до 4, точно неизвестно. 

В целом, идея конеткома когнитома приводит нас к другому нейро-физиологическому вопросу, вопросу пространственной локализации. Существуют две противоположных точки зрения. Согласно первой точки зрения, высшие когнитивные функции человека локализованы в конкретных участках головного мозга (например, зрительная кора, отвечающая за распознование образов, зоны, отвечающие за лексику и грамматику)

// пример о том, как войны помогали этим исследованиям, когда у людей был поврежден головной мозг, и он не мог выполнять определенную функцию
//(исследования аффазии через поврежденные отделы мозга военных)
// есть зона отвечающая за грамматику, человек, с поврежденной зоной связно говорил, но совершенно аграматично. 

// На землю прибыла цивилизация превосходящая нас. Оказалось, что до 20 века наши пути развития были идентичны. Но когда появилась машина и сбила человека, мы придумали ПДД, а они изобрели квантовую телепортацию.

Вторая позиция -- мозг это единое целое, все связано со всем, но в этом едином целом возникают объекты, которые никак геометрически не связаны с зонами, то есть, расположены в разных местах. //Морфно-... теория

В целом, при математическом описании геометрии в сложных системах, и при ответе на майнд-брейн-проблем, мы можем использовать следующие методы: сложные сети как таковые, при этом обычно применяются Community-detection алгоритмы, второй подход: гиперсети (гиперграфы), третий подход: так называемые симплициальные компле'ксы -- базовое понятие топологии, но в том варианте, в котором оно нам нужно, нам его понять достаточно легко. Мы знаем, что такое граф -- G(V, E), пусть кроме ребер мы стали рассматривать вершины более высокого уровня, например грани, то есть мы начинаем рассматривать структуру, которая представляет собой $E times E times E,$ это называется симплициальный комплекс. 

Последний подход: Графоны -- континуальное обобщение графов. 

К критической самоорганизованной системе относится естественный язык и человеческий мозг, соотственно сильный искуственный интеллект тоже будет таковой. К самоорганизованной критической системе относятся системы, которые удволетворяют трем следующим требованиям:

1) Они состоят из гигантского числа взаимодействующих элементов, причем правила взаимодействия между этими элементами сравнительно простые. 

2) В этих системах должны возникать так называемые лавины, когда активация одного элемента влечет активацию второго элемента, второго влечет активацию третьего и так далее. Лавины захватывают существенную часть системы, сопоставимую с ее размерами, или даже систему целиком.  

3) Размеры лавин подчиняются степенным законам распределениям. 

Если это так, то система является самоорганизованной критичной системой. Что же касается языков, то здесь элементарными элементами называются либо сами люди, либо семы -- некий элементарный элемент когнитивного пространства.

Лавина -- любой текст, произнесенный или сказанный представляет собой лавину в языке. 

// от третьяковского до пелевина, прости господи..
