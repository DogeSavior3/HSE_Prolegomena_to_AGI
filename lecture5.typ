= Лекция 5. 

// Метод Брюснера (Pruessner)

Метод Прюснера (self-organized criticality) заключается в том, что при специально выбранных координатах различные выборки, порожденные одним и тем же распределением, дают один и тот же вид распределения. Графически на экране монитора эти зависимости совпадают друг с другом, и мы получаем явление, которое в статистике называется Collapse данных. Соответственно, этот метод выходят далеко за пределы анализа степенных распределений, но Прюснер рассматривал конкретно Collapse данных степенных распределений.

Более того, Прюснер работал с распределениями вида 

$ P(x) = cases(
  ..", "x < x_min,
  C dot X^(- alpha) g(x "/" x_c)", " x >= x_min
) $

где С -- константа нормализации, ... $x_c -$ характерный размер элементов выборки (максимальное значение элемента в выборке), $g(x "/" x_c) -$ функция горба (hunch) и присутствует в большинстве реальных степенных распределений. Если вы возьмете реально степенное распределение и запишите его в двойном логарифмическом масштабе, вы получите что-то вроде :

*рисунок*

Степенное распределение обычно предполагает бесконечное число наблюдений, 

Метод Прюснера лучше прошлых методов, так как может работать с горбами.

Метод Прюснера базируется на двух китах:

1 кит) Экспоненциальный биннинг - когда мы говорим о биннинге распределений данных, мы привыкли к равномерному биннингу, все бины имеют одинаковую длину. Для степенных распределений это не самая лучшая идея, для них лучше применять бины не одинаковой длины, а именно бины, длина которых растет экспоненциально с увеличением значения $X$.

Это достаточно понятно, потому что у степенного распределения большая часть данных будет сконентрирована в окресности максимума, но какой-то объем данных, в силу того, что хвост у него тяжелый, будет сконцентрирован дальше. 

*гистограмма*

2 кит) Collapse данных. Допустим мы угадали х альфа и икс критическое, то для различных выборок, порожденных одним и тем же распределением, наши зависимости в координатах $p' = P(X) dot x^(-alpha), x slash x_c = x'$

Прюснер работал с модельными данными, то есть с выборками, порожденными моделями самоорганизованных критичных систем (самыми сложными из сложных систем). В реальных ситуациях, когда у нас есть всего одна выборка, то, если она достаточно большая, мы берем подвыборки из нее (случайно выбранные, 95%, 90%, 80%), и на этих подвыборках моделируем ситуацию с многими выборками, распределениями. При этом надо ясно понимать, что для каждой из этих выбранных выборок $x_c$ будет разным, что обеспечивает нам Collapse данных.

Мы берем разные выборки (экспоненциально отбиннингованные), и оцениваем (одним из предыдущих методов) параметр $alpha$. Мы будем обозначать эту оценку $alpha'$, она является очень примерной. Поэтому мы строим наши зависимости в наших координатах $p', x'$. Если нулевая гипотеза о том, что мы действительно имеем дело со степенным распределением верна, то мы получаем следующую картинку: 

*рисунок*

У нас есть участок ниже $x_min$ с шумом, у каждой выборки он свой, у нас есть наклонный участок прямой, который отвечает $C dot x^(- alpha)$, и у нас есть характерный изгиб, который Плюснер назвал Landmark. В силу того, что выборки у нас разные, мы получаем разные положения всех трех участков (они сдвинуты друг на друга). С другой стороны, по-скольку $alpha'$ получаена с помощью какой-то грубой оценки, то мы имеем дело с наклонным участком прямой, но то, что этот участок есть, является первый признак того, что распределение, все же, степенное. 

Третий шаг: мы должны оценить истинное значение $alpha$ и $x_c$. Если нам это удастся, то вместо множества графиков, как на предыдущем рисунке, мы получим один график характерного вида, иными словами все графики коллапсируют в один.

*рисунок*

В этом графике будет участок меньше $x_min,$ далее горизонтальный участок прямой, указывающий на то, что расрпеделение является степенным, и характерный landmark, отвечающий функции горба $g$. Для осуществления коллапса, перехода от верхнему графику к нижниму, нужно совершить две операции: повернуть график таким образом, чтобы график стал горизонтальным, тем самым получая истинный $alpha, $ и выбрать характерный масштаб таким образом, чтобы все горбы совпали друг с другом (они действительно совпадут). 

Это можно делать вручную, но Прюснер рекомендует использовать МНК, где в качестве данных выступают положения максимумов горбов и значения в этих максимумах. 

#pagebreak()

Достоинства метода: 

Метод работает с реалистичными степенными распределениями, включающими функцию $g$.

Метод позволяет оценить не просто параметр такого степенного распределения ($x_min, alpha, x_c$), но и проверить нулевую гипотезу о том, что распределение действительно является степенным. Появление этого горизонтального участка прямой является критерием проверки. 

Метод позволяет провести goodness-of-fit test, что дорогого стоит.

Недостатки:

Поскольку он базируется на разных подвыборках, он требует весьма больших выборок, что не всегда возможно в реальных задачах. 

Замечание: теория работы со степенными распределениями является развивающийся областью статистики, и в общем-то задача проверки распределения на степенность является открытой задачей. При практическом использовании целесообразно использовать несколько методов проверки распределения на степенность и делать выводы о степенности, если все три метода дадут положительный ответ с более или менее одинаковыми значениями $x_min, x_max, alpha.$





Конетком и Когнитом -- это понятия, введенные в теорию сильного интеллекта академиком Константином Анохиным. 

Конетком -- совокупность нейронов головного мозга человека вместе со совокупностью их аксонно дендридных связей, носят название конеткома (не трудно догадаться, что это некая сложная сеть со всеми особенностями, присущими сложным сетям, с которыми мы уже знакомы)


// (аксонно дендридных связей -- каждый нейрон человека состоит из ..... сигналы в головном мозге передаются через такие аксонно дендритные связи. между ними есть синноптическая сеть, которая заполнена нейромедиаторами, сила связи между двумя нейронами определяется концентрацией нейромедиаторами в этой щели. радость -- повышение нейромедиаторов в щелях.
// Реальное обучение в реальном головном мозге это не изменение концентрации нейромедиаторов, это сама структура. все эмоции -- отмирание нейронов и новые связи)???

Когнитом -- мы можем в том или ином смысле исследовать конетком, но внутреннему наблюдению нам доступны только ментальные состояния человека. Эти состояния также образуют сложную сеть, которая носит название когнитома, и уже принадлежит области психического (не материального). 
#pagebreak()
Здесь мы сталкиваемся с вечной проблемой нейро-физиологии, которая носит название Mind-Brain problem. Проблема заключается в том, что взаимодействие вполне реальных вещей не вполне понятным нам образом порождает психические явления, то есть явления, относящиеся к сфере духовного. 

Скажем одно, гипотеза Анохина заключается в том, что когнитом представляет собой совокупность когитов, то есть, некоторых временно возникающих совокупностей нейронов головного мозга. Ансамблей нейронов. С математической точки зрения это приводит к понятию сложной гипер-сети, то есть, сложной сети (графа) вершинами которой являются другие сложные сети. ее верхний уровень - когнитом, ее самый нижний уровень -- конетком, но есть промежуточные уровни, которых от 1 до 4, точно неизвестно. 

В целом, идея конеткома когнитома приводит нас к другому нейро-физиологическому вопросу, вопросу пространственной локализации. Существуют две противоположных точки зрения. Согласно первой точки зрения, высшие когнитивные функции человека локализованы в конкретных участках головного мозга (например, зрительная кора)

Вторая позиция -- мозг это единое целое, все связано со всем, но в этом едином целом возникают объекты, которые никак геометрически не связаны с зонами, то есть, расположены в разных местах. //Морфно-... теория

//(исследования аффазии через поврежденные отделы мозга военных)
// есть зона отвечающая за грамматику, человек, с поврежденной зоной связно говорил, но совершенно аграматично. 


В целом, при математическом описании геометрии в сложных системах, и при ответе на манд-брейн-проблем, мы можем использовать следующие методы: сложные сети как таковые, при этом обычно применяются Community-detection алгоритмы, второй подход: гиперсети (гиперграфы), третий подход: так называемые симплициальные компле'ксы -- базовое понятие топологии, но в том варианте, в котором оно нам нужно, нам его понять достаточно легко. Мы знаем, что такое граф -- G(V, E), пусть кроме ребер мы стали рассматривать вершины более высокого уровня, например грани, то есть мы начинаем рассматривать структуру, которая представляет собой $E times E times E,$ это называется симплициальный комплекс. 

Последний подход: Графоны -- континуальное обобщение графов. 

К критической самоорганизованной системе относится язык и .., соотственно сильны искуственный интеллект тоже будет таковой. К самоорганизованной критической системе относятся системы удвол. трем след. требованиям

1) Они состоят из гигантского числа взаимодействующих элементов, причем правила взаимодействия между этими элементами сравнительно простые. 

2) В этих системах должны возникать так называемые лавины, когда активация одного элемента влечет активацию второго элемента, второго влечет активацию третьего и так далее. Лавины захватывают существенную часть системы, сопоставимую с ее размерами, или даже систему целиком.  Размеры лавин подчиняются степенным законам распределениям. Если это так, то система является самоорганизованной критичной. Что же касается языков, то здесь элементарными элементами называются либо сами люди, либо семы -- некий элементарный элемент когнитивного пространства 

Лавина -- любой текст, произнесенный или сказанный представляет собой лавину в языке. 

// от третьяковского до пелевина, прости господи..


