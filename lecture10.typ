= Лекция 10. 

В прошлый раз мы показали, что всякая нейросетевая модель представляет собой дискретный вариант нейродифференциального уравнения. Точнее, ее прямой ход -- это нейродифференциальное уравнение, ее обратный ход, отыскание функции весов сети, суть есть задача вариационного исчисления (оптимального управления). 

То, в каком виде у вас сформулирована задача, представляет собой ситуацию, когда мы подаем на вход сети ровно одно наблюдение ($x_0$), что является неестественной ситуацией. Нормальная ситуация заключается в том, что мы подаем на вход некоторую выборку $xi_1, dots, xi_n$, полученные из некоторого распределения $p_0(x)$, а получаем некоторую выборку $(eta_1, dots, eta_n)$, полученной из распределения $ p_1(x)$.

В отличии от теории вероятности в статистике рассматриваются не только теоретические функции, но и эмперические функции .........

Собственно говоря, вся статистика заключается в том, что мы пытаемся построить некий аналог теоретической величины, такой, что этот аналог восстановим по выборке (является функией выборки) и он в заданном смысле близок к неизвестной теоретической величине. Он хорош в статистическом смысле. 

//распределение знает только Господь 

Можем ли мы восстановить распределение выборки? Отсортируем выборку $xi_1, dots, xi_n$ по возростанию и получим вариационный ряд $xi^*_1, dots, xi^*_n$. Построим следующую функцию:

$ accent(F, "^")_n (x) = cases(
  0",  "x < xi_1^*,
  l/n", "  x = xi_i^* ,
  1",  "x > xi_n^*
) $

Подробнее см. лекцию 4, где мы строили эмперическую функцию таким же образом. (+  рисунок)

Такая функция является хорошим приближением к истинной функции распределения $F(x)$. Более того, для этой функции доказаны весьма сильные математические свойства: 

1) $p l i m " "accent(F, "^")_n ->_(n -> infinity) F(x) $ -- Теорема Гливенто-Кантелли. Это означает, что эимперическая функция распределения является тем более сильным приближением к истинной функции, чем больше размер выборки $N$.

Соответственно, мы можем утверждать, что если мы имеем дело с большой выборкой, то мы можем рассматривать восстановленную по выборке функцию распределения как хороший аналог, замену истинной функции распределения, которую мы не знаем.

Более того, для эмперической функции распределения Колмогоровым было доказано еще более сильное утверждениe. Если мы рассмотрим статистику $K S$ вида 

$ K S = sqrt(n) sup_(- infinity < x < + infinity) |F(x) - hat(F)_n (x)| $

То полученная случайная величина будет иметь одно и то же распределение для всех функций $F(x)$, так называемое распределение Колмогорова. Оно даже выразимо в виде элементарных функций. 

Это означает, что мы можем угадывать истинную функцию распределения. 
//ее знает только Господь, но мы ее не знаем.

Пусть у нас есть некое нейродифференциальное уравнение:

$ accent(x, dot) = f(x(t), w(t)) $

Которое представляет собой некий объект, описывающий динамику процесса обучения. Соответствующий дискретный вариант, каким бы способом дискретизации он не был получен, (методом Эйлера или Ранги-Кутта и тд.) мы будем считать его ранговую аппроксимацию (?)

Если на фазовом пространстве данного нейродифференциального уравнения (на пространстве, на котором определен $x, x in RR^n$) задано некое распределение $P(x),$ то оно будет меняться с течением времени $t$ под действием потока этого нейродифференциального уравнения. Как собственно говоря под действием потока любого дифференциального уравнения. 

То есть на самом деле имеем $P(x, t)$.

Здесь возникают два вопроса:

1) Что будет происходить с функцией $P(x)$ под действием потока нейродифференциального уравнения? Как она будет меняться?

2) Что мы хотим от этой функции $P(x)$? Как она должна соотноситься с нашими выборками ($xi_1. dots, xi_n$) и ($eta_1, dots, eta_n$)? Ответить на второй вопрос легче. 

Мы хотим, чтобы в момент времени $t = 0$, то есть, $P(x,0)$ как можно меньше уклонялась от эмперической функции $hat(P)_0(x)$. 

По той же логике на выходе сети $t = T$, $P(x, T) || hat(P)_1 (x)$. ($P(x, T)$ как можно меньше уклонялось от эмперической функции $hat(P)_1 (x)$.)

Хотим выборку на входе преобразовать в выборку на выходе, чтобы .........

#pagebreak()

Дивергенция Кульбака-Лейблица (Kullback, Leiblez)

$ D_(K L) (P(x) || q(x)) = integral q(x) ln (p(x))/(q(x)) $

Такая функция не .........

$P(x,t)$ чтобы реализовывать процедуру обучения данного нейродифференциального уравнения должна удовлетворять следующему свойству:

$ lambda_1, D_(K L) (P(x, 0) || hat(P)_0 (x)) + lambda_2 (P(x, T) || hat(P)_1 (x)) -> min_(P(x, t)) $

Где $1 >= lambda_1, lambda_2 >= 0, lambda_1 + lambda_2 = 1$, часто берут просто $lambda_1 = lambda_2 = 1/2$

//пересмотреть для интуиции, подумать

Это классическая задача вариационного исчисления. 

В отличии от классической постановки безусловного вариационного исчисления данная постановка имеет 4 ограничения:

$ "1)" integral P(x, t) d x = 1" " forall t in [0,1]; P(x,t) > 0 $

$ "2)" cases(
  accent(x, dot) = f(x(t), w(t)),
  x(0) = x_0
) $

3) $P(x,t)$ меняется под действием потока не произвольно, а некоторым вполне определенным образом. Давайте выясним, как же меняется $P(x)$ под действием потока дифференциального уравнения. Это уравнение Перрона-Фробениуса. УПФ.

Вначале обратимся к дискретному случаю (он проще). Рассмотрим отображение фазового пространства $RR^n$ в себя.

$ x_(i + 1) = f(x_i), f: RR^n -> RR^n $

Рассмотрим один шаг данного отображения. $y = f(x)$. На самом деле это просто замена координат в пространстве $RR^n$. Предполагаем, что $f(x)$ дифференцируема. Выбор системы координат произволен. Можем взять любые, которые получаются друг из друга диффоаморфн....

К сути рассматриваемого явления система координат не имеет. Физические законы независимы от системы координат. Вероятности и качественные свойства не зависят от системы координат. 
// Вероятности придумал Бог, координаты придумал человек

#pagebreak()
Возьмем произвольную точку $y$ и некую ее бесконечно малую окрестность $ [y - (Delta y)/2, y + (Delta y)/2] $

Так же рассмотрим $ x_i : [x_i - (Delta x)/2, x_i + (Delta x)/2] $ 

При этом мы хотим $P(x) -> q(y)$

$ q(y) Delta y = sum_i P(x_i) Delta x $

Эта запись говорит о том, что вероятности сохраняются. При этом очевидно, что длины взятых окрестностей тоже будут меняться (для разных $f$)

Разделим последнее равенство на $Delta y$, и устремим $Delta x -> 0$

$ q(y) = sum_i p(x_i) (Delta x)/(Delta y) $

По теореме об обратной функции получаем 

$ q(y) = sum_i p(x_i) slash f'(x_i) $

Нам будет удобно переписать это выражение пользуясь свойствами $delta$-функции. 

В математике кроме обычных привычных нам функций существуют так называемые "обобщенные" функции, которые представляют собой некое обобщение привычного понятия функции, сохраняющего его свойства, но не представимого в терминах отображения. Такие функции получаются естественным путем, причем естественным с точки зрения приложений, прикладной математики, и естественным с точки зрения чистой математики. С точки зрения чистой математики в рамках функционального анализа доказывается полнота пространства функций, интугрируемых в терминах $L^2$, то есть всех таких функций, для которых $ integral^b_a f^2(x) d x < infinity $ 

#pagebreak()
Но сюда попадают и другие весьма интересные функции. Одна из них $delta$-функция. Формально она определяется следующим образом:

$ delta(x) = cases(
  0", " x!= 0,
  ?", " x = 0
) $
$ integral delta(x) d x = 1 $

Вся квантовая механика построена на этой функции. 

.....................................

Мы только что сказали, что пространство $L^2$ полно, значит предел ряда последовательности таких функций тоже принадлежит $L^2$. Что мы знаем про этот предел? Он равен нулю во всех точках, не равных нулю, он уйдет к бесконечности в точке 0, но интеграл равен единице. 
