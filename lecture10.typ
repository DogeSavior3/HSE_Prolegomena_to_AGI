= Лекция 10. Вариационное исчисление. 

В прошлый раз мы показали, что всякая нейросетевая модель представляет собой дискретный вариант нейродифференциального уравнения. Точнее, ее прямой ход -- это нейродифференциальное уравнение, ее обратный ход, отыскание функции весов сети, суть есть задача вариационного исчисления (оптимального управления -- когда вариация и сама функция могут быть разрывны в конечном числе точек). 

То, в каком виде у вас сформулирована задача, представляет собой ситуацию, когда мы подаем на вход сети ровно одно наблюдение ($x_0$), что является неестественной ситуацией. Нормальная ситуация заключается в том, что мы подаем на вход некоторую выборку $(xi_1, dots, xi_n)$, полученные из некоторого распределения $P_0(x)$, а получаем некоторую выборку $(eta_1, dots, eta_n)$, полученной из распределения $P_1(x)$.

В отличии от теории вероятности в статистике рассматриваются не только теоретические функции распределения, но и их эмперические аналоги.

Собственно говоря, вся статистика заключается в том, что мы пытаемся построить некий аналог теоретической величины (рассмотренной в теории вероятности), такой, что этот аналог восстановим по выборке (является функией выборки) и он в заданном смысле близок к неизвестной теоретической величине. Он хорош в статистическом смысле.

//распределение знает только Господь, мы не знаем

Можем ли мы восстановить распределение выборки? Отсортируем выборку $xi_1, dots, xi_n$ по возростанию и получим вариационный ряд $xi^*_1, dots, xi^*_n$. Построим следующую функцию:

$ accent(F, "^")_n (x) = cases(
  0",  "x < xi_1^*,
  l/n", "  x = xi_i^* ,
  1",  "x > xi_n^*
) $

Подробнее см. лекцию 4, где мы строили эмперическую функцию таким же образом. (+  рисунок)

Такая функция является хорошим приближением к истинной функции распределения $F(x)$. Более того, для этой функции доказаны весьма сильные математические свойства: 

1) $p l i m " "accent(F, "^")_n ->_(n -> infinity) F(x) $ -- Теорема Гливенто-Кантелли. Это означает, что эимперическая функция распределения является тем более сильным приближением к истинной функции, чем больше размер выборки $N$.

Соответственно, мы можем утверждать, что если мы имеем дело с большой выборкой, то мы можем рассматривать восстановленную по выборке функцию распределения как хороший аналог, замену истинной функции распределения, которую мы не знаем.

Более того, для эмперической функции распределения Колмогоровым было доказано еще более сильное утверждениe. Если мы рассмотрим статистику $K S$ вида 

$ K S = sqrt(n) sup_(- infinity < x < + infinity) |F(x) - hat(F)_n (x)| $

То полученная случайная величина будет иметь одно и то же распределение для всех функций $F(x)$, так называемое распределение Колмогорова. Оно даже выразимо в виде элементарных функций. 

Это означает, что мы можем угадывать истинную функцию распределения.

Пусть у нас есть некое нейродифференциальное уравнение:

$ accent(x, dot) = f(x(t), w(t)) $

Которое представляет собой некий объект, описывающий динамику процесса обучения. Соответствующий дискретный вариант, каким бы способом дискретизации он не был получен, (методом Эйлера или более продвинутым методом численного интегрирования Ранги-Кутта или другими) мы будем считать его аппроксимацию.

Если на фазовом пространстве данного нейродифференциального уравнения (на пространстве, на котором определен $x, x in RR^n$) задано некое распределение $P(x),$ то оно будет меняться с течением времени $t$ под действием потока этого нейродифференциального уравнения. Как собственно говоря под действием потока любого дифференциального уравнения. 

То есть на самом деле имеем $P(x, t)$.

Здесь возникают два вопроса:

1) Что будет происходить с функцией $P(x)$ под действием потока нейродифференциального уравнения? Как она будет меняться?

2) Что мы хотим от этой функции $P(x)$? Как она должна соотноситься с нашими выборками ($xi_1. dots, xi_n$) и ($eta_1, dots, eta_n$)? Ответить на второй вопрос легче. 

Мы хотим, чтобы в момент времени $t = 0$, то есть, $P(x,0)$, она как можно меньше уклонялась от эмперической функции $hat(P)_0(x)$. 

По той же логике на выходе сети $t = T$, $P(x, T) || hat(P)_1 (x)$. ($P(x, T)$ как можно меньше уклонялось от эмперической функции $hat(P)_1 (x)$.)

Хотим выборку на входе преобразовать в выборку на выходе так, чтобы преобразование производилось данной нейросетевой архитектурой. 

//делить можно по-братски, по-честному, по-справедливости

#pagebreak()

Дивергенция Кульбака-Лейблица (Kullback, Leiblez)

$ D_(K L) (P(x) || q(x)) = integral_(RR^n) q(x) ln (p(x))/(q(x)) $

Такая функция не удовлетворяет требованиям метики, но она онастолько эффективна и популярна, что является методом по умолчанию. 

Мы должны сказать, что наша функция $P(x,t)$ для того чтобы реализовывать процедуру обучения данного нейродифференциального уравнения должна удовлетворять следующему свойству:

$ lambda_1 dot D_(K L) (P(x, 0) || hat(P)_0 (x)) + lambda_2 dot (P(x, T) || hat(P)_1 (x)) -> min_(P(x, t)) $

Где $1 >= lambda_1, lambda_2 >= 0, lambda_1 + lambda_2 = 1$, часто берут просто $lambda_1 = lambda_2 = 1/2$

$q(x)$ и $p(x)$ в данном случае заданы, интеграл дает определенное число. Мы функции $P(x,t)$ дали в соответствие значение функционала, и мы хотим найти такую $P(x,t), $ которая бы минимизировала данный функционал.  

Это классическая задача вариационного исчисления. 

В отличии от классической постановки безусловного вариационного исчисления данная постановка имеет 3 ограничения:

$ "1)" forall t in [0,1], P(x,t) > 0 arrow.r.hook integral P(x, t) d x = 1 $

$ "2)" cases(
  accent(x, dot) = f(x(t), w(t)),
  x(0) = x_0
) $

3) $P(x,t)$ меняется под действием потока данного нейродифференциального уравнения не произвольно, а некоторым вполне определенным образом. Давайте выясним, как же меняется $P(x)$ под действием потока дифференциального уравнения. Это уравнение Перрона-Фробениуса. УПФ.

Вначале обратимся к дискретному случаю (он проще). Рассмотрим отображение фазового пространства $RR^n$ в себя.

$ x_(i + 1) = f(x_i), f: RR^n -> RR^n $

Рассмотрим один шаг данного отображения. $y = f(x)$. На самом деле это просто замена координат в пространстве $RR^n$. Предполагаем, что $f(x)$ дифференцируема необходимое количество раз. Выбор системы координат произволен. Можем взять любые, которые получаются друг из друга диффиаморфными преобразованиями.

К сути рассматриваемого явления система координат не имеет. Физические законы независимы от системы координат. Вероятности и качественные свойства не зависят от системы координат. 

Как меняются вероятности при смене системы координат? Давайте формализуем эту идею, построим математическую модель этого утверждения:

// Вероятности придумал Бог, координаты придумал человек
// Дюма три мушкетера -- сколько теорем соченил бы Декарт не будь декарт французским офицером

Возьмем произвольную точку $y$ и некую ее бесконечно малую окрестность $ [y - (Delta y)/2, y + (Delta y)/2] $

Где $Delta y$ бесконечно мало. Так же рассмотрим ее прообраз: $ x_i : [x_i - (Delta x)/2, x_i + (Delta x)/2] $ 

У нас есть отображение $f$, оно отображает точки $y$ в точки $x_i$. $P(x)$ при этой замене координат преобразуется в $q(y)$, при этом мы требуем:

$ q(y) Delta y = sum_i P(x_i) Delta x $

Эта запись говорит о том, что вероятности сохраняются. При этом очевидно, что длины взятых окрестностей тоже будут меняться (для разных $f$)

Разделим последнее равенство на $Delta y$, и устремим $Delta x -> 0$

$ q(y) = sum_i p(x_i) (Delta x)/(Delta y) $

По теореме об обратной функции получаем 

$ q(y) = sum_i p(x_i) slash f'(x_i) $

Нам будет удобно переписать это выражение пользуясь свойствами $delta$-функции. 

В математике кроме обычных привычных нам функций существуют так называемые "обобщенные" функции, которые представляют собой некое обобщение привычного понятия функции, сохраняющего его свойства, но не представимого в терминах отображения. Такие функции получаются естественным путем, причем естественным с точки зрения приложений, прикладной математики, и естественным с точки зрения чистой математики. С точки зрения чистой математики в рамках функционального анализа доказывается полнота пространства функций, интугрируемых в терминах $L^2$, то есть всех таких функций, для которых $ integral^b_a f^2(x) d x < infinity $ 

Но сюда попадают и другие весьма интересные функции. Одна из них $delta$-функция (Дирака). Формально она определяется следующим образом:

$ delta(x) = cases(
  0", " x!= 0,
  ?", " x = 0
) $
$ integral_RR delta(x) d x = 1 $

Вся квантовая механика построена на этой функции. Откуда она берется? Давайте рассмотрим последовательность вполне приличных функций: это функции, которые равны $n$ на промежутке $1/n, $ и равны нулю на остальной области определения. Эти функции интегрируемы с квадратом, каждая из них принадлежит пространству $L^2,$ интеграл для каждой равен единице.

Мы только что сказали, что пространство $L^2$ полно, значит предел ряда последовательности таких функций тоже функция, которая принадлежит $L^2$. Что мы знаем про этот предел? Этот предел существует, он равен нулю во всех точках, не равных нулю, он уйдет к бесконечности в точке 0, но интеграл равен единице. Это и есть наша дельта-функция. 

//важная байка про функциональный анализ
