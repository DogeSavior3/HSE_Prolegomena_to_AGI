= Лекция 9. 

Нужно идею дифференциальных уравнений развить до тех идей, которые будут использоваться в сильном искусственном интеллекте.

Качественные и количественные теории обыкновенных дифференциальных уравнений.

Количественная теория обыкновенных дифференциальных уравнений предполагает, что базовым объектом исследования является само по себе обыкновенное дифференциальное уравнение, и представляет собой по сути некую сводку, алгебраическое решение такого рода уравнения. Классический подход того, как излагать дифференциальное уравнение. Этот подход сталкивается с двумя возражениями, когда речь идет о реальных, практических задачах, связанных с ОДУ.

Подавляющее число ОДУ не имеет аналитического решения. (не мы не можем его найти, а его просто нет, доказан факт его отсутствия) И в этом случае мы выходим за рамки классических ОДУ и приходим в другую математическую дисциплину: численные методы или вычислительная математика. Это рассказ о том, как решать ОДУ с помощью компьютера. Термин носит название численно интегрировать ОДУ. 
// Задачник филлипова о том, как составлять дифференциальные уравнения.
// Проблема заключается в том, что коэфициент по которому считается диффур не всегда реально посчитать, и обращение к какому-нибудь росстату является лукавством 
// когда речь идет о химических-физических задачах, предметы подвержены каким-то простым законам и нам легко их описать, а если система сложная, то законов тоже много и они сложны

// Если $x(t), y(t)$ это количество жертв и хищников, то выполняется следующее
// $ accent(x, dot)(t) = alpha x(t) - gamma x(t) y(t) $
// $ accent(y, dot)(t) = beta y(t) + gamma x(t) y(t) $
// 
// Отсюда следует следующее:

При исследовании реальных процессов, описываемых дифференциальными уравнениями, нас интересуют не сами уравнения, а .... Глобально эта идея носит название ..... моделирование. А применительно к ОДУ, она дает нам качественную теорию ОДУ. Качественная теория работает с качественными свойствами и базовым ее объектом являются так называемые потоки ОДУ, о которых мы поговорим позже.

Если мы хотим работать со сложными системами, то мы должны работать, во-первых, с численными методами, во-вторых, с качественной теорией ОДУ. Наша базовая интенция.

// Мир нелинеен означает что такие дифференциальные уравнения не имеют линейных зависимостей 

Численные методы решения ОДУ. В подавляющем большинстве аналитических решений не существует и нужно их искать численно. Здесь возможны два варианта: Задача Коши :

$ (d y)/(d x) = f(x, y), y in RR^1 $
$ y(0) = y_0 $

И краевая задача. Рассмотрим, как решать задачу Коши в одномерном случае, но в общем случае алгоритм такой же, просто матричный. 

Разобьем область определения $y: [0, x_max]$ на маленькие кусочки длины $h$. Стоит подчеркнуть, что $h$ являются малыми, но не бесконечно малыми, около $10^(-2), 10^(-3)$ в сравнении с $x_max$. Рассмотрим первые из таких промежутков, лежащие от $0$ до $h$.  Перепишем наш диффур в виде $d y = f(x, y) d x$. Сначала применим оператор интегрирования $integral_0^h$. Применим к нашему диффуру формулу Ньютона-Лейбница на этом участке. 

$ y(h) - y(0) = integral_0^h f(x,y) d x $
$ y(h) = y(0) + integral_0^h f(x,y) d x $

Если нам удастся оценить этот интеграл, то мы сможем найти $y(h)$. 

Самый простейший метод оценки -- посчитать площадь трапеции под функцией. *Вставить фото с семинара*

Давайте рассмотрим площадь прямоугольника, она будет примерно равна $ h dot f(0, y(0)) $

Это выражение мы можем вычислить, значит мы знаем $y(h)$. Давайте рассмотрим следующий кусочек $[h, 2h]$. Такой же логикой мы можем найти $y(2h),$ зная $y(h).$ В результате последовательного применения численного интегрирования мы получаем последовательность значений исходной функции, удовлетворяющей граничным условиям и дифференциальному уравнению. Находим решение для этой функции в конечном множестве точек. Так как $h$ по сути параметр, мы можем сделать эти значения сколь угодно частыми. Если мы нарисуем график, то увидим, что аналитическое и численное решения совпадают. Таким образом, нет большого смысла искать аналитическое решение в реальных задачах. 

Следует иметь в виду, что любой численный метод обладает погрешностью, вносимой на каждом шаге численного интегрирования. Чем дальше мы двигаемся, тем большую погрешность мы внесли в решение. Для рассмотренного метода Эйлера погрешность на каждом шаге составляет $O(h).$ Значит в самом неудачном случае двигаясь по промежутку мы можем внести погрешность сравнимую с значением функции. 
#pagebreak()
К счастью, существуют гораздо более эффективные методы численного интегрирования. Самым базовым является метод Рунге-Кутты (4-го порядка). Он тоже основывается на достаточно логичных принципах, мы функцию разности между истинной и аппроксимированной функцией расскладываем в функцию Тейлора. Погрешность, которую дает этот метод на каждом шаге: $O(h^s),$ где $s -$ взятый порядок. 
// коэффициенты в диффуре будут менее точными, чем этот метод
// ДЗ: посмотреть вывод метода Рунге-Кутты

Отступление. Нейро-дифференцальные уравнения.
// Какое-то время назад в ии была революция, когда все перешли с малых моделей на глубокое обучение, далее будет революция, заключающаяся в использовании данных диффуров

... Resnet, $x_(n + 1) in RR^s = x_n + f(x_n)$ -- состояние следующего слоя. Состояние следующего слоя -- функция активации от текущего слоя. В теории нейро-дифференцальных уравнений мы говорим: давайте считать, что процесс протекания информации от входа в нейронную сеть к выходу разворачивается .......... 

Для первого слоя мы находимся в $n = 0, n = 1, dots, n = N, Delta t = 1$

Это означает, что у нас есть две величины : одна - $N -$ число слоев, а вторая $- T -$ (физическое) время, которое протекло от момента подачи информации до момента выхода. До этого мы их не различали, потому что считали, что $T = N dot Delta t$. Дальнейшая логика изложения: что такое переход от неглубокого обучения к глубокому? Ранее $N$ было небольшим, позже мы обсудим почему, .........., но произошла некая революция, модели глубокого обучения, где мы сказали, что $N -$ весьма большая величина. Нейро-диффурщики сказали: давайте доведем процесс до конца. Давайте не считать время $T$ фиксированным, а $N$ устремим к бесконечности. $T - $ константа, $N -> infinity,$ следовательно, $Delta t -> 0$. Перепишем состояния слоев:

$ (x(t + Delta t) - x(t))/(Delta t) = f(x(t)) $

Давайте применим оператор предельного перехода к обоим частям равенства:

$ lim_(Delta t -> 0) (x(t + Delta t) - x(t))/(Delta t) = f(x(t)) $

Получаем ОДУ: $(d x)/(d t) = f(x(t))$. Получаем обычную задачу Коши, остается только добавить, что $x(0) = x_0$. Следуя традиции классического машинного обучения мы смотрим не на истинный объект (нейро-диффур), а какое-то приближение: $ x_(n + 1) = x_n + f(x_n) $

Если внимательно посмотреть на формулу Эйлера, то мы обнаружим, что на самом деле мы для исследования истинного объекта используем старый метод Эйлера. Сейчас идет стремительный переход к нейро-дифференцальным уравнениям, мы поговорим о них далее. 

#pagebreak()

То, что мы сейчас расписали -- это лишь то, что называется прямой ход нейронной сети. Мы написали уравнение, которое описывает изменение состояния. Здесь у нас слоев бесконечно количество, но сути это не меняет. Понятно, что здесь мы не рассмотрели главного: в нашей нейронной сети нет весов. При рассмотрении нейро-дифференцальных уравнений мы, конечно же, должны рассматривать и вектор-функцию весов $w(t)$, где $w(t)$ есть некий аналог матрицы (тензора) весов нейронной сети, который получается из него тем же предельным переходом $Delta t -> 0$. Тогда мы имеем, что наш диффур зависит не только от состояния $x(t),$ но и от $w(t), $ которая и подлежит определению.

$ (d x)/(d t) = f(x(t), w(t))$

Мы будем определять ее следующим образом: 

У нас есть некая функция ошибки (потери), которая характеризует состояние сети ..........

$ L(x(t), x^*) -> min $

Например квадратичная функция ошибки:

$ L = 1/2 [x(t) - x^*]^2 $
$ d x = f(x(t), w(t)) d t $
$ integral_0^t d x = integral_0^t f(x(t), w(t)) d t $
$ x(T) = underbrace(x(0), = x_0) + integral_0^t f(x(t), w(t)) d t => $
$ => L = 1/2 [ x_0 + integral_0^t f(x(t), w(t)) d t - x^*]^2 $


