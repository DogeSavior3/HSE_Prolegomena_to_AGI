= Лекция 17. Методы кластеризации.

Прежде всего обратимся к описанию алгоритма кластеризации $z$-векторов. На прошлой лекции мы сформулировали, что это должен быть не просто алгоритм кластеризации, но алгоритм, удовлетворяющий определенным требованиям:

+ Число кластеров априори неизвестно и должно определяться в процессе алгоритма кластеризации. 
+ Форма кластера априори неизвестна, и алгоритм должен быть способен выделять кластера самой различной формы, потому что в данных, порождаемых прогнозированием на основе кластеризации, мы можем в одной кластеризации встретить кластера весьма различных, прихотливых форм. 
+ Размерность кластеризируемых данных может быть если и не совсем большой, но во всяком случае больше трех. Размерность выборки, число векторов, подлежащих кластеризации, может быть очень объемным. 

Соответственно, среди гигантского множества алгоритмов кластеризации, найдется мало алгоритмов, подходящих нашим требованиям.

// существующих что существуют, и не 

Рассмотрим один из них: алгоритм Уишарта (Wishart). Алгоритм Уишарта принадлежит одновременно двум классам алгоритмов кластеризации. Во-первых: это кластеризация на основе функции плотности (density-based clustering). Во-вторых: это кластеризация на основе графов (graph-based clustering). Соответственно, позволяет сочетать достоинства обоих классов кластеризации. А именно:

Алгоритмы на основе плотности обычно позволяют определять число кластеров в процессе кластеризации. А вот алгоритмы на основе графов позволяют работать с кластерами любой формы. Сочетание этих двух подходов позволяет удовлетворить двум нашим главным требованиям. 

Пусть мы имеем выборку ${X_i}_(i = 0)^n, X_i in RR^s$

Мы хотим построить четкую кластеризацию, то есть, мы хотим разбить множество $S = {X_i}_(i = 0)^n$ на непересекающие множества $c_i$ таким образом, чтобы:

$ union.big_i c_i = S ; forall i != j " " c_i union c_j = diameter $

Обычно к этой постановке добавляют некую меру качества кластеризации -- некий функционал $f:{c_i} -> [0,1]$, которые тем выше (тем ниже), чем больше значение функции $f$ на данной кластеризации.

#pagebreak()

В чем заключаются алгоритмы кластеризации на основе плотности?

#image("../images/lecture_17_images/picture1.png", width: 60%)

Самое простое, что можно сделать -- провести линию (для данного графика, например, значение $y = 20$), соответственно, получим 2 кластера по границам пересечения сгущений точек с этой линией. 

У алгоритма Уишарта немного другой подход к определению кластера, но в целом повторяется идея с построением чего-то похожего на функцию плотности и поиском "не плоских" участков. Так работает большинство density-based clustering алгоритмов вроде HDBSCAN и подобных. 

Алгоритмы на основе графов заключаются в построении некого специфического графа, который носит название графа данных (data graph, proxinity graph). Вершины этого графа соответствуют всем точкам выборки, а ребра этого взвешенного графа соединяют те вершины, которые близки друг к другу. Понятие близости можно определять по-разному. Типов этих графов данных существует около двух десятков. Если граф данных взвешенный, то вес обычно является расстоянием между вершинами. 

#align(center, "Алгоритм Уишарта")

0) Для каждой точки выборки $X_i$ считаем расстояние до $k$-го ближайшего соседа $rho_k (x_i)$ и считаем объем шара $v_k (x_i)$ с центром в точке $x_i$ и радиусом, равным $rho_k (x_i)$. После чего мы считаем значимость наблюдения $x_i$ как $p_k (x_i) = k/(W v_k (x_i))$. После этого идет сортировка $x_i$ по убыванию их значимости, либо по возростанию объема $v_k (x_i)$. Это два достаточно времяемких действия, но существуют алгоритмы быстрого поиска этих статистик. 

Последующие шаги стартуют с первых из отсортированных точек. Что такое точка с наибольшей значимостью? Это те точки, для которых $k$-ближайшие находятся близко. Такие точки принадлежат областям сгущения. Если мы стартуем с таких точек, это будет означать, что мы стартуем с самых центров будущих кластеров. 

Мы предполагаем, что на $i$-ом шаге алгоритма строится некоторый частичный подграф графа данных $G_i = angle.l V_i, E_i angle.r $, предполагаем, что на $i$-ом шаге этому подграфу принадлежат первые $i$ точек нашей отсортированной выборки наблюдений. При этом установлены все необходимые ребра и необходимые вершины, и отнесены к тому или иному классу. 

Факт отнесенности к классу мы будем обозначать как $w(x_i) = l >= 0$. Если $l>0, $ то $l -$ номер класса. Если $l = 0$, это означает, что $i$-е наблюдение принадлежит межкластерному шуму. 

Шаг $i+1$) Прибавляем к нашему графу вершину, которая относится к наблюдению $x_(i+1)$, и посмотрим на $c (x_(i+1)): {x_j in V_i : rho(x_j, x_(i+1)) <= rho_k (x_j)}$, то есть, множество всех вершин, которые уже принадлежат к $G_i$, кроме того наш новый элемент $x_(i+1)$ принадлежит их $k$-ближайшим окрестностям. Рассмотрим возможные виды такого множества: 

1) $c(x_(i + 1)) = diameter.$ В этой ситуации мы создаем новый кластер и добавляем наблюдение $x_i,$ увеличиваем счетчик кластеров на единицу. 

2) $c(x_(i+1))$ состоит из элементов одного и того же кластера, $w(x_j in c(x_(i+1))) = l$, и этот кластер не сформирован, тогда мы относим $x_(i+1)$ к этому классу. Если же кластер уже сформирован (его создание завершено и мы более не можем относить к нему другие элементы), тогда мы относим наблюдение $x_(i+1)$ к межкластерному шуму.

3) $c(x_(i + 1)) != diameter$, более того, в нем есть точки, относящиеся к разным кластером, тогда каждый из кластеров проверяется на значимость, то есть мы считаем $ max_(x_i, x_j in c) |p_k (x_i) - p_k (x_j)| $ и сравниваем с $h.$ Если максимум меньше, чем $h$, то кластер не значимый. Если больше, то значимый. 

#image("../images/lecture_17_images/picture2.jpeg", width: 70%)

В кластере есть разброс определенной величины между точкой максимальной значимости и точки минимальной значимости. C помощью этой формулы мы как бы ищем резкое падение значимости, выходит, через $h$ мы задаем порог кластеров. 

Пусть число значимых кластеров это $z(h).$ Если $z(h) > 1,$ то мы объявляем все значимые кластера сформированными. Элементы всех незначимых кластеров относим к межкластерному шуму. Более того, саму точку $x_(i+1)$ тоже относим к межкластерному шуму. 

Иначе пусть $z(h) = 1, l_1 != 0$ (где $l_i$ отсортированы $l_1 < l_2 < dots < l_m$). Тогда мы оставляем только один значимый кластер, вливаем в него все незначимые кластера, тем самым ликвидируя их, и относим к значимому кластеру значение $x_(i+1)$. 

Итерация по шагам алгоритма продолжается до тех пор, пока мы не исчерпаем все точки выборки, то есть, не осуществим кластеризацию. При том алгоритме, который сформулирован, каждая точка будет отнесена к тому или иному кластеру. Обычно после завершения шагов алгоритма, кластера, размер которых меньше какого-то заданного алгоритма, мы относим эти точки в межкластерный шум.

Достоинства и недостатки:

Достоинства очевидны: определение числа класетров в автоматическом режиме, работа с кластерами любого типа. 

Недостатки: 

+ Времяемкий алгоритм для больших выборок.
+ Алгоритм зависит от двух параметров $k$ и $h$, и качество результата работы алгоритма зависит от обоих параметров. Обычно их порядок достаточно понятен, но, тем ни менее, иногда нужен многократный запуск при разных значениях $k$ и $h$ с вычислением некой меры кластеризации. 

У этого алгоритма есть и другие достоинства, кроме определения числа кластеров. 

// хрен редьки не слаще

Такой подход к пониманию кластера обычно приводит к тому, что на реальных данных значительная часть данных (70-80-90%) относится к межкластерному шуму. Кластера получаются маленькими и компактными. На практике в следствие этой особенности его любят сочетать с другими алгоритмами кластеризации (например, определить число кластеров и запустить к-средних). Но мы не решаем задачу кластеризации, мы решаем задачу прогнозирования, и в этом смысле оказывается, что алгоритм Уишарта действительно находит мотивы. 

#pagebreak()

Возвращаемся к прогнозированию на основе кластеризации. Мы хотим добиться того, чтобы с ростом числа шагов вперед, на которое мы хотим получить прогноз, ошибка прогнозирования не росла экспоненциально, как предполагает первый закон Ляпунова. С этой целью было предложено несколько идей. 

Первая идея заключается в так называемых шаблонах. Теорема Таккенса нам говорит о том, что число элементов в $z$-векторе должно быть больше некоторого числа, целой части округления вверх $2d+1$, где $d-$ размерность некоторого геометрического объекта, странного аттрактора, который этот ряд породил. Но теорема Таккенса не говорит нам, каким образом эти вектора должны быть сформированы. В классическом варианте они формируются просто из последовательных наблюдений. Было предложено формировать вектора не только из последовательных наблюдений, но и наблюдений, сделанных по некоторому шаблону. 

Представим "гребешок" с некоторыми выломанными зубьями, допустим, между зубьями расстояния $k_1, dots, k_l.$ Прикладываем его к нашему ряду, тогда под зубьями будут некоторые наблдения, то есть наблюдения $y_0, y_(0 + k_1), y_(0 + k_1 + k_2), dots, y_(0 + k_1 +dots + k_l)$

$ z_i = (y_i, y_(i + k_1), y_(i + k_1 + k_2), dots, y_(i + k_1 +dots + k_l)) $

Получаем $z$-вектора путем сдвига такого "гребешка" от начала к концу. Для каждого шаблона вида $k_1, dots, k_l$ получаем выборку, выполняем кластеризацию и получаем для каждого шаблона свой набор мотивов. 

Для каждой точки, значение в которой мы хотим спрогнозировать, мы берем все шаблоны, которые у нас есть, и прикладываем все шаблоны так, чтобы последний элемент был точкой, зачение которой мы хотим спрогнозировать, а остальные "зубья" стояли в точках, для которых значение известно, либо в точках, значения которых мы уже спрогнозировали. 

Мы формируем вектор наблюдений из тех точек, которые нам известны, и сравниваем этот вектор наблюдений $z^*$ с обрезанными шаблонами:

$ z_i (alpha) : rho(z^*, "Trunc"(z_i (alpha)) < epsilon) $

Получаем снова вектор прогнозов. Однако этой идеи с шаблонами оказалось недостаточно. На следующей лекции обсудим, чем дополнить данный метод.
