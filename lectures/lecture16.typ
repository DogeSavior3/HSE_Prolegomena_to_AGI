= Лекция 16 

Для того чтобы преодолеть фундоментальный закон, который не позволяет нам прогнозировать за горизонтом прогнозирования было предложено несколько концепций, которые в совокупности дают необходимый эффект. 

Давайте отметим, что эти концепции сочетаются с любым методом прогнозирования, с любым подходом к прогнозированию, однако излагать их мы будем на основании так называемого прогнозирования на основе кластеризации (Predictive clustering). Подход был предложен в 90-е годы прошлого века и показал свою работоспособность в сравнении со всеми остальными методами, подходами, при прогнозировании хаотических временных рядов на много шагов вперед. Метод базируется на предположении, что для рассматриваемого временного ряда все переходные процессы завершены. И движение происходит в окрестности аттрактора. Поскольку ряд хаотический, то аттрактор странный. 

Что такое переходные процессы? В теории прогнозирования и в теории динамических систем, вообще все наблюдаемые на земле процессы делятся на две категории: переходные и установившиеся. Установившиеся процессы означают, что движение системы протекает в окрестности некоторого аттрактора, не обязательно странного. И соответственно сложность прогноза определяется именно сложностью этого аттрактора как геометрического объекта. Переходные процессы означают, что движение протекает вне окрестности любого аттрактора системы, что система еще не успела притянутся ни к одному из своих аттракторов, что система находится на пути к установившемуся процессу. Или на пути между двумя установившемися процессами. В последнем случае говорят о жесткой бифуркации или о катастрофе системы. Естественно, в реальных системах переходные процессы кратковременны, а установившиеся процессы долговременны. Поэтому разумно принебрегать переходными процессами, что мы и сделаем. Разумно считать, что они завершены и мы движемся в окрестности некоторого аттрактора, в нашем случае странного. 

Если это так, то мы в наших дальнейших рассуждениях можем опереться на теорему Пуан-Каре () о возвращении траектории. Теорема гласит следующее: для всякой динамической системы, определеяемой своим потоком $g^t$, и для всякой точки $x$ ее инвариантного множества, существует момент времени $T$, что $g^T (x)$, то есть точка, в которой окажется $x$ через какое-то время, принадлежит окрестности точки $x,$ сколь бы мала не была эта окрестность.

$ g^T (x) in U(x) $

#pagebreak()

Соответственно, это означает, что любая траектория любой системы, отвечающая установившемуся процессу, рано или поздно вернется в окрестность самой себя. Пройдет рядом с той точкой, через которую она уже проходила, причем "рядом" может означать любое весьма малое расстояние $epsilon>0$. При этом для регулярных траекторий это обычно совпадение. А для хаотических траекторий доказано прямо противоположное. Траектория никогда дважды не пройдет через ту же самую точку, сколько малой бы не был $epsilon$. Но по теореме Пуан-Каре она пройдет через $epsilon-$окрестность точки $x$. При этом время $T$ нам неизвестно, Пуан-Каре доказал лишь его существование. В подавляющем большинстве ....

Например, на ряде Лоренса (стандартный бенчмарк всех моделей прогнозирования хаотических рядов), $T$ равномерно распределенная случайная величина. Однако ....... На этот факт мы и обопремся. 

Что с нашей точки зрения говорин теорема Пуан-Каре? Если мы рассмотрим хаотическую систему и установившийся процесс, то любой участок ряда отвечает движению по некоторой области странного аттрактора. 

Если мы от геометрического представления в фазовом пространстве перейдем к динамическому представлению в виде ряда, то мы получаем удивительную речь: если кусок ряда встречался, то через какое-то $T$ этот кусок повторится (не совсем он, но что-то отличное от него на $epsilon$). 

Вообще говоря, вероятность попадания траектории в один и тот же гиперкубик странного аттрактора равна инвариантной мере динамической системы, о которой мы говорили в прошлый раз. Для одних гиперкубиков эта вероятность больше, для других меньше. С практической точки зрения это означает, что если мы возьмем какой-то кусок (chunk) ряда, то в одних случаях он будет многократно повторяться (с точностью до $epsilon$), а в других случаях он будет повторяться редко, возможно мы вообще не сможем увидеть некоторые куски ряда. Если мы имеем дело с бесконечным рядом, то теорема Пуан-Каре гарантирует нам, что мы для каждого кусочка найдем его соответствие. К счастью, мы имеем дело с конечным рядом, то, соответственно, некоторые чанки могут и не повториться. Тем ни менее, для достаточно большого ряда и для достаточно хорошей хаотической динамической системы, мы будем наблюдать повторяемость некоторых кусочков ряда. 

Отсюда следует весьма любопытный вывод: если мы разрежем ряд на кусочки, то есть на наши $z$-вектора, то мы можем это множество векторов кластеризировать тем или иным образом, и эта операция кластеризации будет разумной. Кластера будут соответствовать областям странного аттрактора. 

#pagebreak()

Центройды таких класетров будут называться мотивами (motifs). Пусть наши $z$-вектора и наши мотивы имеют размерность $l$. Мы рассматрриваем наш временной ряд и в какой-то момент наблюдения прекращены. Мы хотим получить прогноз для следующей точки этого временного ряда. Если мы возьмем участок временного ряда, предшествующий точке завершения наблюдений, длины $l-1$, и сравним его с каждым из мотивов, обрезанным на единичку, то если окажется, что этот наблюдаемый вектор $z^*$ близок к какому-то из обрезанных мотивов: $ rho(z^*, "Trunc" xi_e ) < epsilon $ То, соответственно, мы можем в качестве прогноза выбрать последний элемент этого обрезанного мотива. Среди множества всех существующих мотивов совсем необязательно выбирать один единственный (например, ближайший) мотив. Может оказаться, что на расстоянии, не превышающем $epsilon$, у нас окажется несколько, и даже достаточно много таких обрезанных мотивов. Это позволяет нам получить для нашей точки не один прогноз $hat(y)_(t+1)$, но целое множество возможных прогнозных значений, которое носит название множества возможных прогнозных значений. Обозначается $hat(S)(y_(t+1))$. Вообще говоря, это неплохой результат по двум причинам:

В теории, где получение даже одного прогнозного значения тяжело, получить целое множество....

Если у нас есть несолько оценок одной и той же величины, то это всегда хорошо со статистической точки зрения, можно их, например, усреднить, и получить более хорошую в каком-то смысле величину, чем каждая из исходных оценок. 

У этого подхода есть три недостатка:

Структура множества прогнозных значений не будет тривиальной, как нам бы хотелось. 

Множество $z$-векторов необходимо кластеризировать. Это означает, что мы должны подобрать эффективный алгоритм кластеризации. В зависимости от данных, от структуры, сложности, кластеризируемости данных, мы должны применять различные алгоритмы кластеризации, которых придумано огромное множество.

Многомерность данных. 

//книжка aggarwal reddy

Разговор про кластеризацию, я пока не понял, что из этого нам нужно, допишу позже.

Алгоритм кластеризации должен уметь работать с кластерами разного типа.
