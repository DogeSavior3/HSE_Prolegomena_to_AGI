= Лекция 13. Теорема Колмогорова-Арнольда

Почему когда мы решаем эту задачу мы используем то представление, которое использовалось в ИОМК? 

$ u(x,y) = sum^n_(i = 0) h_i (x) g_i (y) $

Любому человеку понятно, что это не самое общее представление функции от двух переменных. Вообще говоря общее представление -- функция от $n$ переменных.

Это подводит нас к теореме Колмогорова-Арнольда и ее применении при решении нелинейных уравнений в частных производных. 

Вопрос о представлении функции многих переменных через суперпозицию функции одной переменной составляет существо 13-ой проблемы Гильберта: в самом начале 20 века Девид Гильберт сформулировал 25 проблем, самых сложных проблем в чистой прикладной математике на тот момент времени. Три из них не решены до сих пор. Еще несколько переформулированы таким образом, чтобы быть более трактуемыми в формальных математических терминах. Однако для 13-ой проблемы решение было найдено нашими соотечественниками Андреем Николаевичем Колмогоровым и его учеником Владимиром Игоревичем Арнольдом. Оно выражается в знаменитой теореме Колмогорова-Арнольда (KST, Kolmogorov Superposotion Theorem), которая гласит следующее:

Всякая непрерывная функция $n$ переменных представима в виде:

$ u(x_1, dots, x_n) = sum_(i = 1)^(2n + 1) chi_i (sum_(j = 1)^n phi_(i j)(x_i)) $

Где внутренние функции $phi_(i j)$ и внешние функции $chi_i$ представляют собой непрерывные функции одной переменной. На вопрос Гильберта был дан положительный ответ. Во-первых, следует подчеркнуть, что в формулировке теоремы Колмогорова-Арнольда фигурирует точное равенство (не аппроксимация). Во-вторых, необходимое число функций весьма мало. Необходимое число внутренних функций равняется числу переменных, $n$. Для функции от двух переменных это число 2. Внешних функций требуется $2n + 1$. В рамках этой теоремы Колмогоров и Арнольд показали, что внутренние функции могут быть одинаковыми для всех непрерывных функций $u$. А вот внешние функции $chi_i$ уже специфичны для функции $u$, представление которой мы ищем. 

Указанное представление является предельно сильным. И породило гигантское количество всевозможных приложений. В частности, следствие из теоремы Колмогорова (переформулировка):

$ u(x_1, dots, x_n) = sum_(i = 1)^(2n + 1) chi_i (sum_(j = 1)^n alpha_(i j) phi_(j)(x_i)) $

Здесь $phi_j$ по прежнему одинаковы для всех функций $u$, а вот внешние функции $chi_i$ и коэфициенты $d_(i j)$ уже специфичны для функции $u$. 

Такое представление является основой всех нейросетевых представлений. Формально мы здесь показали, что мы можем построить нейронную сеть, которая состоит всего лишь из двух слоев. Нам нужно только лишь выбрать в качестве функции активации внутреннего слоя функции $phi_j$, а в качестве функции активации внешнего слоя функции $chi_i$. Тогда обучив нейронную сеть таким образом, чтобы веса, ведущие от первого слоя ко второму были в точности равны этим Колмогоровским коэфициентам $alpha_(i j)$.

Соответствующие утверждение появилось на заре нейронных сетей, носит название теоремы Hech-Nielsen, сейчас это тренд в нейронных сетях, им активно занимаются. Называются KAN сети, Kolmogorov-Arnold's Networks.
//Забыли о теореме Хеч-Нилсон на 30 лет, сейчас стала трендом.

Возвращаясь от нейронных сетей к теореме Колмогорова-Арнольда, практическое применение сдерживалось двумя обстоятельствами:

+ В оригинальном доказательстве Колмогорова существование как внутренних так и внешних функций просто доказывалось, но мы не имели представления о том, как их реально строить. Но это только половина беды. 

+ Внутренние и внешние функции представляли собой специфический математический объект. Они были всюду непрерывны, но нигде не дифференцируемы. В математике такого рода функции носят названия Чертовых Лестниц, devil's staircase. 

Первая проблема была решена уже в 60-х годах прошлого века. Советско-американский математик David Scprecher построил алгоритм построения внутренних функций в явном виде. 
//Позже Knoppen нашел ошибку.

Однако гораздо более сильным результатом в этом направлении были результаты шведского математика Хедберга, который показал, что представление Колмогорова-Арнольда имеет место не только для тех конкретных функций, которые доказал Колмогоров, но для весьма большого числа функций из пространства $C[0, 1]$. 

Те функции, для которых это верно, они "плотны" в множестве всех непрерывных функций. С практической точки зрения это значит, что если вы берете функцию $phi$, то если вам удастся найти функцию $chi$, то вы получаете точное Колмогоровское приближение. Утверждение носит название теоремы Каханне. 

Второй вопрос представлялся существенно более болезненным. Примерно в одно время с Колмогоровым было показано, что если мы потребуем от функции не только непрерывность, но еще и аналитичность (сильная гладкость), то утверждение Колмогорова-Арнольда неверно. Возникает естественный вопрос: имеет ли это практическое применение?

//Если ряд тейлора сходится к самой функции, то она аналитична

Для наших целей, а именно для построения полной бифуркационной картины этот спор не важен по одной причине: если мы рассмотрим множество всех функций $chi_i$, которые фигурируют в представлениях и рассмотрим те функции $chi_i$, которые не просто фигурируют в представлении функции $u$, но фигурируют в представлении тех функций, которые являются решениями наших уравнений в частных производных, то поскольку функция $u$ дифференцируема, то дифференцируемыми являются все функции $chi_i, phi_j$. 

С другой стороны по алгоритму Шпрехера и по базовой идее Колмогорова-Арнольда внутренние функции представляют собой предел некоторого итерационного процесса. На каждом шаге этого итерационного процесса мы знаем значения этих функций в конечном числе точек, а потом мы можем считать их гладкими в той степени, в которой нам захочется. Посколько мы являемся прикладными математиками, то мы в общем-то всегда имеем дело с функциями из численных методов, соответственно, с функциями известными в конечном числе точек. С формальной точки зрения здесь нет никакого противоречия. Однако с точки зрения вычислительной математики это может приводить к весьма неприятным последствиям, ведь численно удерживаться на этом подмногообразии функций нам не удастся, мы все время соскакиваем и оказываемся с численной точке зрения в заборах, wild functions, что затрудняет численный счет. Некие результаты в этой области были получены. 

Нам понадобится еще одно следствие из теоремы Колмогорова-Арнольда о суперпозиции. Уже в 21-ом столетии Rauf Doss доказал очевидное следствие: 

$ u(x_1, dots, x_n) = sum_(i = 1)^(2n + 1) chi_i (product_(j = 1)^n phi_j (x_j)) $

В этой ситуации мы получаем произведение. Из этого утверждения получаются два следствия, рассмотрим их в терминах функции от двух переменных (для упрощения)

$ u(x,y) = sum_(i = 1)^5 chi_i (phi_1 (x) dot phi_2 (y)) $

1) Давайте разложим эту функцию в ряд Тейлора в окрестности нуля (можем и в окрестности любой заданной функции, но сделаем в окрестности нуля, так как для любой сложной системы существует решение близкое к нулю, в физике это, например, термодинамическая ветвь)

$ approx sum_(i = 1)^5 chi_i (0) + sum_(i = 1)^5 chi'_i (0) dot phi_1(x) dot phi_2 (y) $

Получаем метод Кантаровича. Чем больше слагаемых удержите, тем точнее будет аппроксимация. Ничего нам не мешает обобщить Итерационный Обобщенный метод Кантаровича. Не будем ничего расскладывать в ряд Тейлора. Будем работать с изначальной, точной, формулировкой. Но тогда мы имеем суперпозицию функций от одной переменной, просто они более сложные. 

$ u(x,y) = sum_(i = 1)^5 chi_i (z) (phi_1 (x) dot phi_2 (y)) $

$chi_i (z)$ являются функциями одной переменной. Если мы применим логику ИОМК и будем отыскивать в рамках аналогичного итерационного процесса и $phi_1 (x)$ и $phi_2 (y)$ и $chi_i$, то мы получим на каждой итерации такого процесса разрешающие соотношения для вышеуказанных функций одной переменной, причем эти разрешающие соотношения будут представлять собой краевые задачи для ОДУ, которые мы знаем как решать. 

_Замечание_. Самое важное, что следует подчеркнуть: в независимости от того какую формулировку теоремы Колмогорова-Арнольда мы выберем и какой метод, аналогичный ИОМК мы не построим, мы все равно получаем какое-то последовательное применение метода Ньютона для решения каких-то краевых задач для ОДУ.

Соответственно, вся теория фиксации точки бифуркаций, определение их типа, отыскание исходящих из них ветвей решения, соотнесение с теорией катастроф, построение полной бифуркационной картины получается естественным образом. Для одномерного случая мы это просто знаем. А в случае многих переменных через различные формулировки теоремы Колмогорова-Арнольда все сводится к функциям от одной переменной. 

Таким образом мы достигли решения нашей базовой задачи, а именно решения задачи построения полной бифуркационной картины для нелинейных уравнений в частных производных, данной постановки задачи. Именно к этому классу математических объектов относится задача машинного обучения. Соответствующие разделы машинного обучения являются некой численной аппрокисмацией соответствующего непрерывного случая.

_Замечание_. Мы говорим о вариационных постановках. Проблeма заключается в том, что, зачастую, в дифференциальных постановках задач машинного обучения встречаются производные нечетных порядков. 

$ (partial P)/(partial t) (x,t) $

Мы не можем ничего проварьировать, для решения такой постановки. Решение было найдено тремя немецкими математиками Jordon, Kinderlehrer, Olto, JKO. Идея заключалась в достаточно простой вещи: на том месте, функциональная вариация которого должна давать нам производную, должен стоять объект, который при вариации дает аппроксимацию данной производной. Разностная аппроксимация. 

$ (u(x,t)-u(x, t - Delta t))^2/(2h) $

При этом формально мы считаем что при движении по времени, координате $t$, мы имеем наши вариационные постановки, определенные на малых, конечных промежутках времени длины $Delta t$. Тогда приступая к поиску функции на промежутке $(t, t + Delta t)$, мы уже знаем функцию $u(x, t + Delta t)$, она не подлежит варьированию на этом промежутке. Тогда  вариация такого выражения даст нам $ (u(x,t) - u(x, t - Delta t))/h $

Что близко к производной $(partial u)/(partial t).$ Погрешность такого приближения: $O(Delta t)$. Делая его малым, мы получаем вариационную постановку, к которой можно применять ИОМК. Это та идея, которую предложили JKO. 

Не обязательно рабски следовать их идее. Мы можем ввести соответствующее слагаемое функционалом, формально, и просто его варьировать. Тогда в результате вариации мы получим, соотвестсвенно, не ОДУ, но так называемые дифференциальные уравнения с задержкой. Это класс дифференциальных уравнений, в которых кроме самой функции и ее производных так же фигурируют функции, зависящие не от времени $t,$ а от времени $t - a,$ где $a-$ фиксированная константа.

Если мы говорим о сильном искусственном интеллекте, то мы говорим о умении перехода между состояниями системы. Сильный искусственный интеллект -- способность управлять переходами между состояниями системами.

Эти переходы с математической точки зрения дают нам теорию биффуркаций для нелинейных ОДУ, выраженных вариационной постановкой (и не только). 

Применение ИОМК позволяет нам строить бифуркационные картины и заниматься решением как прямых, так и обратных задач и задач управления. 
