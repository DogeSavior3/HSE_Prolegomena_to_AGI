= Лекция 6. Сложность

Самый важный тег для нас -- сложность. Если мы говорим о сильном искусственном интеллекте, то мы должны говорить о сложности.

Понятие сложности в математике может определяться по-разному.

Существует три центральных подхода:

1) Сложно то, что сложно сгенерировать (Колмогоровская сложность). Если для данного массива данных мы должны построить такую машину Тьюринга, что (какая-то цель) -- определение сложности. Практически слабо применимо. 

2) Сложно то, что сложно предсказать.(Предсказательная сложность). Базовая идея, что если мы не можем предсказать поведение системы, то она сложна. 

3) Сложно то, что сложно выучить. (Размерность Вафника-Червоненкиса, $V C$). 

Эти все рассуждения о сложности не являются чисто теоретически. Золотое правило интеллекта: сложность обрабатываемых данных должна равняться сложности системы, которая их обрабатывает. Для того, чтобы оно работало, необходимо хоть как-то определить сложность.

Когда речь идет об обычных нейронных сетях, мы видим следующую картину:

#image("../images/lecture_6_images/picture1.jpeg", width: 65%)

Если вы увеличиваете сложность, то до какого-то момента ошибка обобщения падает (ошибка обобщения -- оишбка на тестовой выборке, ошибка обучения -- ошибка на тренировочной выборке), есть некий оптимум сложности, который наилучшим образом соответствует данным, с которыми мы работаем. Если мы продолжаем увеличивать сложность, мы получаем возростающий участок. Второй кусочек связан с революцией в машинном обучении в связи с появлением глубокого обучения. Но с чем это связано теоретически пока никто не понимает. Возможно это связано с теорией самоорганизации, о которой мы поговорим позже. Как интерпретировать восходящие и снисходящие участки? 

#pagebreak()
Предположим, что у нас есть какая-то истинная функция:

#image("../images/lecture_6_images/picture2.jpeg", width: 50%)

Мы не имеем всей функции, у нас есть выборка, снятая с этой функции. Допустим, сложность системы проще сложности данных, нашей системе просто не хватит ресурсов, чтобы более менее точно соответствовать системе (функция 1). Из-за чего получаем большую ошибку. 

Если мы попадаем куда-то в окрестность оптимума, то мы сможем более менее точно пройтись по кривой и более менее точно ей соответствовать. (функция 2)

Переобучение -- контринтуитивное свойство, в данном случае мы получаем так называемые wild-functions, они достаточно хорошо проходят через каждый элемент выборки, но засчет большого количество степеней свободы мы получаем такой забор (функция 3). Такая функция не имеет никакого отношения к истинной функции, поэтому ошибка обобщения становится гигантской. 

$""$

Прежде чем переходить к предсказательной сложности (predictive_complexity), стоит поговорить про информацию и информационную энтропию.

Шэноновская информация. Давайте рассмотрим два вероятностных события:

Событие А: Профессор N вошел в кабинет.

Событие Б: Профессор N убил студента.

Какое событие несет больше информации? Почему?

Вероятность второго события существенно ниже. После этого Шэнон сказал:

1) $I = f(1/p)$

Где $I$ - информация, $p-$ вероятность.

2) Пусть есть два независимых события, тогда $I = I_1 + I_2, f(1/p_1 1/p_2) = f(1/p_1)f(1/p_2) $

Функция логарифма удовлетворяет такому свойству. С другой стороны можно доказать, что единственная функция, которая удовлетворяет такому свойству -- логарифм. Тогда $I = ln 1/p = - ln p$

Предположим, что у нас есть система, которая может находиться в $N$ состояниях. И в этих состояниях она находится с вероятностями $p_1, dots, p_n$. Тогда энтропия по Шэнону есть средняя информация, которую мы знаем о такой системе.

$ H = "<"I">" = sum^N_(i = 1) p_i I_i = - sum^N_(i = 1)p_i ln p_i $

Что это означает? Свойства:

Предположим, что мы знаем о системе все, тогда мы можем утверждать, что с вероятностью $p_i = 1$ система находится в определенном состоянии, остальные вероятности равны нулю. Тогда энтропия равна нулю. Значит если мы все знаем о системе, то ее информационная энтропия равна нулю. 

В обратной ситуации, когда мы не знаем ничего о системе, то $p_i$ равновероятны. $ p_i = 1/N => H = ln N => ln N = max(H) $

Из этих двух свойств мы делаем вывод: информационная энтропия -- мера неопределенности системы. Допустим мы получили какую-то информацию, тогда энтропия упадет, и мера полученной информации равна этой разнице.

Как генерировать хаотические ряды? $X_(n+1) = 1 - lambda X_n^2$ -- логистическая зависимость. Это мы обсуждать не будем, просто пример ряда, который генерирует информацию с каждой итерацией. И за небольшой период времени он сгенерирует тонну информации, хотя по сути, никакой смысловой нагрузки она не имеет. В этом заключается одна из проблем шэноновской энтропии.

Введенная таким образом Шэновская энтропия является аддитивной в силу выполнения свойства 2 (про независимые события), более того, можно показать, что она является единственной аддитивной энтропией. Между тем в сложных системах обычно имеют место так называемые не аддитивные энтропии, для которых свойство 2 не выполняется по банальной причине: считается, что не существует двух подлинно информационно изолированнох подсистем. 

Поэтому в сложных системах целесообразно применять не аддитивные энтропии, то есть энтропии, для которых неверно $I = I_1 + I_2$ 

Этих энтропий бывает много, но основые: энтропия Реньи(Renyi) 

$ H_q = 1/(1 - q) ln sum^n_(i = 1) p_i^q $ 

Энтропия Цависа(Tsalis), энтропия Каниадакиса(Kaniadakis), энтропия Шарма-Митталя и так далее.. Открытая проблема, к какой сложной системе какую не аддитивную энтропию применять.

Общее методическое замечание: Сложные системы делятся на классы универсальности, классы смежности. Сложным системам, пренадлежащим одному классу, свойственны некие универсальные свойства.  

Предсказательная сложность. Предположим, что у нас идет некоторый поток данных в неком времени, не обязательно метрологическом, и мы можем выделить в нем некоторый момент времени $t = 0$ и две части данных: прошлое и будущее. Тогда данные, относящиеся к прошлому мы будет обозначать как $X_(p a s t),$ а к будущему $X_(f u t u r e),$ совместную вероятность прошлого обозначим за $P(X_(p a s t))$, совместную вероятность будущего за $P(X_(f u t u r e))$, тогда взаимной информацией между будущем и прошлым будет: 
$ I(X_(f u t u r e), X_(p a s t)) = "<" log_2 (P(X_(f u t u r e) | X_(p a s t)))/P(X_(f u t u r e)) ">" $

Где усреднение производится по совместному распределению $P(X_(f u t u r e), X_(p a s t))$.

Эта величина носит название Девергенция Кульбаха Лейблера (Kulback Leibler). Если мы хотим сравнить между собой, например, две матрицы, мы посчитаем какую-нибудь норму или какое-нибудь расстояние. Это наше действие по умолчанию. А если мы хотим сравнить два вероятностных распределения, то действием по умолчанию является сравнение по Девергенции Кульбаха Лейблера.

По сути мы сравниваем информацию о прошлом (которая позволяет нам говорить о будущем) с $P(X_(f u t u r e)).$ То есть, действительно предсказательную сложность.

Вот математическая мера того, насколько нам сложно предсказать будущее по прошлому. Если мы умножим числитель и знаменатель на $P(X_(p a s t))$, то получим

$ I(X_(f u t u r e), X_(p a s t)) = "<" log_2 (P(X_(f u t u r e) | X_(p a s t)))/P(X_(f u t u r e)) ">" = $ $ =  - "<" log_2 P(X_(p a s t)) ">" - "<" log_2 P(X_(f u t u r e))">" -" "["" -"<"log_2 P(X_(f u t u r e), X_(p a s t))">" ] $

Пусть $p a s t = T', f u t u r e = T;$ тогда по определению нами рассмотренной энтропии:

$ = H(T') + H(T) - H(T + T') $

Таким образом мы свели понятие взаимной информации к некой комбинации информационных энтропий.

Замечание 1:

Выражение, введенное для совместной информации, введеное таким образом, является симметричным. В этом смысле равносложными являются задачи как предсказания, так и подсказания, когда мы прошлое определяем по будущему.

//Футурология: предсказание о будущем, ее инструмент -- forsight, а есть понятие oversight. Мы по имеющейся ситуации смотрим, а какие еще пути развития могли быть. Как правило, наш путь не единственный и далеко не лучший.

В формуле H всегда есть линейная состовляющая и еще одна одна величина.

$ H(T) = H_0 dot T + underbrace(H_S (T), o(T)) $

Первая, линейная, называется экстенсивной энтропией, она связана с течением времени. То есть, любая система подчиняется второму началу термодинамики. В этом смысле энтропия растет всегда, причем растет пропорционально протекшему времени. Это имеет место абсолютно для любой системы. И это в общем то неинтересно. 

Вторая -- субэкстенсивная энтропия, которая ведет себя как $o(T)$ и определяет поведение системы, которая отличает ее от всех остальных.

Мы постараемся показать, что разные системы дают нам принципиально различные функции суб-экстенсивной энтропии как функции времени. Тип этой функции и дает нам класс сложности соответствующей системы. Более того, типов этих функций существует не так уж и много. И это дает нам некоторый первый подход к классификации сложных систем. Пока же нам достаточно отметить, что экстенсивые состовляющие энтропии сокращаются. Соответственно, совместная информация определяется только суб-экстенсивной состовляющей энтропии. 

$ I(X_(f u t u r e), X_(p a s t)) = H_s (T') + H_s (T) - H_s (T + T') $

Предположим, что мы знаем о прошлом все, тогда мы можем устремить $T'$ к бесконечности: 

$ lim_(T' -> + infinity) I(T | T') = H_S (T) = I_(p r e d) (T) $

Тогда мы получим по сути суб-экстенсивную составляющую того, что мы называем будущим. Эта величина и носит название предсказательной сложности.

Нужно обратить внимание на следующий факт: эта величина симметрична относительно обращения прошлого и будущего. Задача предикции и постдикции -- одинакова.  Можно устремить не $T',$ а $T$, получится то же самое выражение.

Здесь можно сказать, что, когда мы говорим о такой хорошей, фундоментальной величине, она должна удовлетворять двум, противоречивым требованиям:

1) она следует из базовых принципов

2) ее нужно уметь измерить экспериментально

Метрика хорошо измеряется.
 
