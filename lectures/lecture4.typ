#import "@preview/cetz:0.3.2": *
#import "@preview/cetz-plot:0.1.1": plot, chart
#set text(font: "New Computer Modern", lang: "ru")

= Лекция 4. Идентификация степенных распределений.

Базовой задачей, при установлении того факта, что граф, с которым мы имеем дело является сложной сетью, является задача идентификация степенных распределений, а именно установление того "простого" факта, что данная выборка порождена степенным распределением. Математически такая задача является задачей математической статистики. Но работа со степенными распределениями не входит в стандартный курс.

Мы имеем выборку, то есть набор н.о.р.с.в $(xi_1,dots,xi_n)_(i i d)$. Мы можем ставить в отношение этой выборки два вопроса:

1) Мы предполагаем, что выборка этих величин порождена неким конкретным распределением, класс которого нам известен. Например, это выборка из нормального распределения. Но мы не знаем параметры этого распределения. Мы хотим проверить гипотезу о параметрах этого распределения. 

$ N(a, sigma^2); H_0 : a = a_0 $

Другой, более важный для нас вопрос:

2) В реальных сложных системах мы обычно не знаем класс распределения, которым порождена наша выборка. Соответственно, второй вопрос, к какому классу распределений принадлежит распределение, породившее нашу выборку.

Мы можем "попытаться" проверить статистическую гипотезу о том, что распределение, породившее выборку, это некое конкретное распределение, за этой выборкой стоит некий конкретный вероятностный закон.

$ H_0: F = F_0 $
При этом мы не ограничиваем себя каким-то конкретным классом распределений. Любая функция, удовлетворяющая требованиям функции распределения.

В первом случае говорят о параметрической статистике, потому что выдвигаемые гипотезы касаются параметров распределения, класс распределения мы знаем. 

Во втором случае говорят о непараметрической статистике, потому что выдвигаемые гипотезы касаются распределения как такового. Иногда употребляют англоязычный термин goodness-of-fit test, проверка гипотезы, насколько данная выборка соответствует данному распределению.
#pagebreak()
При работе со степенными распределениями является первым и более важным является ответ на второй вопрос. Должны ли мы работать с этой выборкой как с выборкой из степенного распределения, должны ли мы предполагать, что мы можем каким-то образом оценить параметры степенного распределения, исходя из того, что мы действительно имеем дело со степенным распределением

Оказалось, что даже классических методов непараметрической статистики недостаточно. Сколько нибудь эффективные методы работы со степенными распределениями появились в последние $15$ лет, поэтому, они обычно не входят в классический курс математической статистики.

Вспомним, что такое степенное распределение:

$ P(x) = C dot x^(-alpha) $

Где $alpha$ является параметром распределения, а $C$ - константой нормализации, гарантирующей нам, что:

$ integral_(- infinity)^(+infinity) P(X) " "d x = 1 $

Соответственно, когда мы говорим о оценке параметров степенного распределения, мы на самом деле оцениваем один параметр -- $alpha$, а $C$ просто определяется из условия.

Какие подходы мы можем указать для решения задачи идентификации, является ли наша выборка выборкой из степенного распределения?

Первый метод носит название Метод Хилла (Hill), он базируется на переходе к двойному логарифмическому масштабу. Если мы прологарифмируем выражение плотности для степенного распределения, мы получим:
$ P(x) = C dot x^(-alpha) $
$ underbrace(ln P, p') = underbrace(ln C, C') - alpha underbrace(ln x, x') $
$ p' = C' - alpha x' $

#pagebreak()

Соответственно, если мы нарисуем в этих новых координатах нашу зависимость, то это должен быть отрезок прямой с отрицательным коэффициентом наклона, в том случае, если верна наша нулевая гипотеза о том, что мы имеем дело со степенным распределением.

#let two(x) = {
  -x + 5
}
#let zero(x) = 0

#canvas( length: 4cm, {
    import draw: *
    set-style(
        stroke: (thickness: 0.8pt, cap: "round"),
        content: (padding: 1pt),
        mark: (fill: black, scale: 1.5)
    )

    plot.plot(size: (1,1),
          x-tick-step: none,
          y-tick-step: none,
          axis-style : "left",
          x-label : $x'$,
          y-label : $p'$,
          {

           plot.add(two, domain: (1, 4), mark: (fill: black))
           plot.add(zero, domain:(0, 0))
})
})
Более формально, можно предложить следующее развитие метода Хилла: давайте оценим параметры $C'$ и $alpha$ с помощью МНК или метода максимального правдоподобия. 

МНК: из выборки имеем $p'_1, dots, p'_n, x'_1, ...,x'_n, $ тогда давайте посчитаем минимум следующей функции:

$ 1/n sum_(i = 0)^n (p'_i - C' - alpha x'_i)^2 -> min $

где $n$ -- размеры выборки. Отсюда, дифференцируя по $C'$ и $alpha$ ($p'_i$ и $x'_i$ нам известны) мы находим выражения для оценки $C'$ и $alpha$, которые минимизируют это выражение. Если окажется, что полученная таким образом оценка (оценка методом наименьших квадратов) действительно делает этот квадратичный функционал малым, то это означает, что, во-первых, мы нашли хорошие оценки этих двух параметров, а во-вторых, что у нас действительно имеет место степенной закон распределения. 

Следующий подход принадлежит трем американским математикам Clauset, Shalizi, Newman, которые, в прочем, опирались на работы двух российских математиков Колмогорова и Смирнова. Так называемая $K S$-статистика. 

По выборке н.о.р.с.в $(xi_1,dots,xi_n), xi_i ~ F(x),$ мы можем построить так называемую

Эмпирическую функцию распределения. В одномерном случае алгоритм построения эмпирической функции распределения $accent(F, "^")_n (x)$ выглядит просто:

Мы сортируем выборку $(xi_1,dots,xi_n)$, по возрастанию, и получаем из нее так называемый вариационный ряд $(xi^*_1,dots,xi^*_n)$

#pagebreak()

$ accent(F, "^")_n (x) = cases(
  0",  "x < xi_1^*,
  l/n", "  x = xi_i^* ,
  1",  "x > xi_n^*
) $

#let fun1(x) = 1
#let fun2(x) = 1/2

#canvas(
    length: 6cm,
    {
        import draw: *
        set-style(
            stroke: (thickness: 0.8pt, cap: "round"),
            content: (padding: 1pt),
            mark: (fill: black, scale: 1.5)
        )

        plot.plot(
            size: (1, 1),
            x-tick-step: none,
            y-tick-step: 1,
            axis-style: "school-book",
            x-label: $$,
            y-label: $$,
            {
                plot.add(zero, domain: (-16, -9), style: (stroke: (paint: black, thickness: 1.5pt)))
                
                plot.add(fun1, domain: (10, 18), style: (stroke: (paint: black)))
                
                for i in range(0, 16) {
                    let fraction = i / 16
                    let func(x) = fraction

                    let start = -10 + i * 1.2
                    let end = -9 + i * 1.2
                    
                    plot.add(func, domain: (start, end), style: (stroke: (paint: black)))
                }
            }
        )
    }
)

Функция $accent(F, "^")_n (x)$ = 0 для всех $x < xi_1^*,$ $accent(F, "^")_n (x)$ = 1 для всех $x > xi_n^*$,  и в каждой точке $xi_j^*$ она совершает скачок на величину $1/n$, если существует только одно значение $xi_j^*$ (нет равных ей) и скачок на $l/n$, если в вариационном ряде встречается $l$ одинаковых значений $xi_j^*$. 

Определенная функция является функцией распределения по определению. через нее мы проведем истинную функцию, которую мы аппроксимировали такой эмпирической функцией. Чем больше выборка, тем точнее такая аппроксимация будет приближаться к истинной функции. Про эмпирическую функцию распределения было доказано два предельно мощных утверждения.

Первое утверждение носит название теорема Гливенто-Кантелли. 

При увеличении выборки до бесконечности, случайная величина $F_n (x)$ сходится по вероятности к $F(x)$ 

$ p l i m " "accent(F, "^")_n ->_(n -> infinity) F(x) $

Второе утверждение носит название теоремы Колмогорова. Если мы рассмотрим статистику $K S$ вида 

$ K S = sqrt(n) sup_(- infinity < x < + infinity) |F(x) - hat(F)_n (x)| $

То полученная случайная величина будет иметь одно и то же распределение для всех функций $F(x)$, так называемое распределение Колмогорова.

#pagebreak()

На основании этой теоремы Колмогорова и его ученика Смирнова был сформулирован, пожалуй, первый критерий в непараметрической статистике.

Мы выдвигаем нулевую гипотезу, что $F$ -- конкретно заданная функция $F_0$:

$ H_0: F = F_0 $

Тогда, если наша гипотеза верна, то $F_0$ и $hat(F)_n$, восстановленная по выборке, должны мало отклоняться друг от друга. Причем, мы можем оценить степень этой малости, а именно мы должны сравнить $K S$ с квантилем распределения Колмогорова.

$ K S " ? " K_(alpha ; n)  $

Где $alpha$ -- уровень значимости, $n$ -- количество степеней свободы.

Если эта величина действительно мала ($K S < K_(alpha ; n)$), то мы не отклоняем нулевую гипотезу. В противоположном случае мы отклоняем нулевую гипотезу и принимаем альтернативную.

Каковы же недостатки критерия $K S$? Для того, чтобы применять критерий $K S$ к выборкам в реальных задачах мы должны знать точное значение параметра $alpha$, а для того, чтобы сколько-нибудь адекватно оценить параметр $alpha,$ мы должны быть уверены, что выборка, с которой мы имеем дело, порождена степенным распределением. На практике мы получаем логический круг. Но Clauset, Shalizi и Newman придумали способ, как из него выбраться. 

Они предложили следующую идею:

1) На практике, степенное распределение в чистом виде встречается редко. Обычно реальные распределения имеют вид

$ P(X) = cases(
  ?", " x < x_(min),
  C dot x^(- alpha)", " x_min <= x <= x_max,
  ?", " x > x_max
) $

В двойном логарифмическом масштабе это выглядит как #image("../images/lecture_4_images/graph.png", width: 40%)

Физически это связано с тем, что сложные системы имеют некие характерные масштабы, для которых и выполняются законы поведения сложных сетей. 

Давайте возьмем $x_min$ в некотором разумном диапазоне значений с некоторым разумным шагом. Для каждого конкретного значения $x_min$ с помощью метода Хилла, либо любого другого метода параметрической статистики, мы оценим значение $alpha$, удержав в вариационном ряде только те значения, которые больше текущего $x_min$. Получив оценку для $alpha$, мы тем самым в точности специфицируем функцию $F_0$ из критерия Колмогорова Смирнова. 

$ F_0 = C dot X^(- hat(alpha)_("Hill")) $

Тогда мы можем для такой функции вычислить значение $K S$ статистики, для функции от $hat(alpha)$ и в конечном случае от $x_min$. Нарисуем график такой зависимости:

#image("../images/lecture_4_images/graph2.png", width: 60%)

Он будет убывающим по очевидной причине -- чем меньше у нас выборка, тем точнее мы можем приблизить нашу функцию данным распределением. Он будет убывающим не монотонно, тогда в качестве значения $x_min$ выбирается значение, при котором функция $K S (x_min)$ достигает своего первого локального минимума. Это некая общая идея, иногда выбирается не первый локальный минимум, а второй или третий, что обычно связано с той ситуацией, что первый локальный минимум является некоторой флуктуацией на убывающем участке такой зависимости.

Тем самым мы получаем оптимальные в некотором смысле оценки $hat(x)_min, hat(alpha)$.

К сожалению, этого оказалось недостаточно для проведения goodness-of-fit теста для степенного распределения. 

Замечание: для $x_max$ процедура аналогичная, но мы двигаемся справа налево. Однако, обычно $x_max$ мало влияет на результат, а вот $x_min$ может быть критичен.

#pagebreak()

Clauset, Shalizi и Newman предложили решение данной проблемы проверки на степенность. Они предложили на ряду с исходной выборкой рассмотреть еще значительное число синтетических выборок, а именно синтетических выборок, порожденных законом распределения:

$ C dot X^(-hat(alpha)), x > hat(x)_min $

как раз с теми оценками, которые мы получили движением по графику $K S$ от $x_min.$ 

Мы берем некоторое степенное распределение, и порождаем выборки. У нас есть много выборок. Для каждой выборки мы считаем $K S$, в том числе для исходной (обрезав $x_min$). В подавляющем большинстве случаев, значение $K S$ для нашей выборки будет больше, чем для синтетических выборок, которые мы сделали таким образом, чтобы они были максимально близки к нашему реальному распределению.

$K S$ статистика показывает, насколько истинное распределение близко к нашей эмпирической функции распределения. 

Если исходная выборка не является степенной, например на самом деле она является нормальной, то $K S$ статистика для будет иметь гигантское значение, намного большее, чем значение $K S$ статистики для синтетических выборок, так как мы будем пытаться сравнивать что-то восстановленное по нормальному распределению с истинно степенным распределением.

Мы замерим процент случаев $p$, для которых:

$ K S_0 < min_j K S_j $

где $K S_0$ -- $K S$ статистика для нашей выборки, $K S_j$ -- для синтетических. Обычно берут порядка тысячи ($j < 1000$).

Если верна нулевая гипотеза о том, что $F$ -- степенное распределение, что за нашей наблюдаемой выборкой стоит степенное распределение, то $p > 10%$.

Этот полуэмпирический критерий служит способом проверки выборки на степенность. Если $p < 10% $, то есть, $p$ близко к нулю, то мы имеем дело не со степенным распределением.

В рамках одного метода первое достоинство -- мы умудряемся и оценить параметр распределения, и ответить на вопрос, действительно ли распределение является степенным. 
Второе достоинство -- метод работает. Недостатки метода: нет теоретического обоснования границы в $10%$, все остальное теоретически обоснованно. 

Второй недостаток: время-емкий процесс. 
