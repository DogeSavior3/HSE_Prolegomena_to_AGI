= Лекция 15. Энтропия Колмогорова-Синая.

Начали изучаение хаотических систем, ввели старший показатель Ляпунова. Ввели понятие Ляпуновского спектра и старшего показателя Ляпунова как его максимального значения. Все значения спектра носят название линейных показателей Ляпунова, потому что они зависят лишь от одной траектории линеализированной системы. Кроме линейных(одномерных) показателей Ляпунова существуют еще объемные, или многомерные показатели Ляпунова. Пусть $u_1(t), dots, u_m(t)$ — линейно независимые решения линеаризации исследуемой системы, тогда ляпуновский показатель порядка $m$:

$ kappa_m (u_1, dots u_m) = lim_(t -> infinity) 1/t ln sqrt(det mat((u_1,u_1), "", (u_m, u_1); dots.v, dots.down, dots.v;
(u_1, u_m), "", (u_m u_m)))  $

Теория говорит нам, что среди всех $kappa_m$ выделяется показать порядка $n$, но в нашем случае $m <= n$. Для одномерного, линейного, случая, называемого случаем общего положения, является ситуация $lambda = lambda_1$, где $lambda_1$ -- старший показатель Ляпунова, и мера тех начальных условий в пространстве $RR^n$, с которых мы можем достигнуть других элементов Ляпуновского спектра, равна нулю. Соответственно, для объемного показателя ляпунова таким значением $kappa_m$ будет $lambda_1 + dots + lambda_m$.

_Замечание_. Это дает нам способ вычисление всего Ляпуновского спектра. Достаточно сказать, что:

$ lambda_j = kappa_j - kappa_(j - 1) $

Особый интерес представляет случай $m = n$. В этой ситуации матрица представляет собой так называемый определитель Вронского $:=$ Вронскиан. При $m = n$ объемный показатель Ляпунова дает нам фундоментальную классификацию всех систем на диссепативные и консервативные. 

Все системы в мире делятся на две категории: диссепативные и консервативные. 

Консервативные системы (от лат. conservatio -- сохраняют) это системы, в которых выполняется один или более законов сохранения, например закон сохранения энергии.

Диссепативные системы (от лат. dissipatio -- рассеивает) это системы в которых все законы сохранения нарушаются. Консервативные системы являются каким-то допущением, но нам легко с ними работать. Есть значительная часть реальных систем, которые нам удобно считать консервативными, в которых мы можем принебречь диссепативными эффектами. Однако следует понимать, что есть системы, где диссепативные эффекты принципальны. (например теория самоорганизации, которая строится на том, что нарушаются законы сохранении энергии, материи, и любой информации, по сути это теория диссепативных систем)

Для консервативных систем $kappa_m = 0.$

Для диссепативных систем $kappa_m < 0$, что дает нам некую характеристику. 

Энтропия Колмогорова-Синая, или почему хаотические системы производят информацию. Мы установили, что все хаотические системы порождают множество траекторий, неустойчивых по Ляпунову. Мы не можем работать с этими траекториями, соответственно, и с этими системами так, как бы мы работали с обычными детерменированными системами, например теми, что описываются системами ОДУ.

// принцип неопределенности герцена-чернышевского, нельзя одновременно отвечать на вопрос кто виноват и что делать, будем отвечать на вопрос что делать

Достаточно долгое время было непонятно, как работать с такого рода системами. Однако в 60-е годы прошлого столетия Колмогоров предложил, что, несмотря на то, что они полностью детерменированные, рассматривать их как системы вероятностные и работать не с отдельными траекториями, а с некоторыми распределениями, порождаемыми этими системами и с некоторыми понятными нам характеристиками (математическим ожиданием, дисперсией, и подобным).

У нас есть фазовое пространство, я разбил его на гиперкубики, диаметр каждого гиперкуба $< epsilon$, далее мы запускаем одну/много траекторий до бесконечности. Если мы будем длить траекторию достаточно долго, то время пребывания траектории в каждом отдельном гиперкубе будет некой фиксированной, мало меняющейся величиной. По сути мы построили какую-то эмперическую функцию распределения. 

Устремив $t -> infinity$, а $epsilon -> 0$, мы получаем некоторую вероятностную меру, которая называется инвариантной мерой некоторой динамической системой. Будем обозначать ее буквой $mu$. Такая инвариатная мера для хаотических систем будет сингулярной, не непрерывной, не дискретной, и не их комбинацией. Но все еще будет вероятностной мерой. Более того, можно показать, что она существует для любой системы, в частности для любой хаотической системы. Последнее утверждение носит названия теоремы Крылова-Боголюбова.

Раз у нас есть вероятностная мера (не связана с теорией вероятности, но интеграл по области равен единице, более того, это мера является единственной подлинной характеристикой такой системы), значит мы можем вычислить некоторые привычные нам характеристики этой меры. 

Первой характеристикой является энтропия динамической системы, или энтропия Колмогорова-Синая. 

Давайте рассмотрим фазовое пространство нашей динамической системы и разобьем его на непересекающиеся подмножества $A_1, A_2, dots, A_n$ и обозначим их совокупности $A_i_1$. Теперь рассмотрим множество с двумя индексами $ A_(i_1 i_2) = A_i_1 inter f^(-1)(A_i_2) $

Где $x_(i+1) = f(x_i), f: RR^n -> RR^n$.

По индукции строим множество $ A_(i_1, dots, i_k) = A_i_1 inter f^(-1)(A_i_2) inter dots inter f^(-k + 1) (A_i_k) $

Рассмотрим следующее выражение:

$ H_k = sum_(i_1, dots, i_k) mu(A_(i_1, dots, i_k)) ln mu(A_(i_1, dots, i_k)) $

Где $mu$ -- инвариантная мера динамической системы, заданная $f$.

Тогда энтропия Колмогорова-Синая определяется как:

$ K S = lim_(epsilon -> 0) lim_(k -> + infinity) [H_(k + 1) - H_k] = lim_(epsilon -> 0) lim_(k -> + infinity) H_k slash k $

Внешний предел можно опустить, дробление происходить за счет $k -> infinity$.

Полученная величина носит название энтропии Колмогорова-Синая, или энтропии соответствующей динамической системы. 
//перепроверить

Для хаотиеческих систем $K S > 0$, для регулярных систем $K S = 0$.

По сути это та скорость, с которой растет информация. 

#align(center, "Прогнозирование за горизонтами прогнозирования")

Рассмотрим временной ряд $y_1, dots, y_t, dots in RR^n$ и поставим цель решить для него задачу прогнозирования, которая формулируется следующим образом:

$ M[y_(t+k) - hat(y)_(t+k)]^2 -> min $

и разобьем его на отрезки длины $s$. В теории их называют z-вектора, а на жаргоне чанки (chunks). $s - $ достаточно небольшая величина. 

$ z_0 = (y_0, y_1, dots, y_(s-1)) $
$ z_1 = (y_1, y_2, dots, y_(s)) $

$ z_s = (y_i, y_(i+1), dots, y_(i + s - 1)) $

Длина этого вектора -- фундоментальная характеристика. Сделаем отступление и поговорим про теорему Таккенса (Takens), или основная теорема теории прогнозирования. Мы сказали, что наблюдаемая динамика является траекторией некоторого неизвестного ОДУ $accent(x, dot) = f(x)$. Базовое предположение всей хаотической динамики заключается в том, что траектории этой системы движутся в окрестности некоторого аттрактора, геометрического центра точек. Поскольку сам объект сложный, мы получаем сложную систему. Называются они странные аттракторы, некоторая фрактальная система. 

Пусть у нас есть размерность аттрактора $n$, тогда мы можем вложить такую траекторию только в объекты размерности $d >= 2n + 1$. Это теорема Уитни (Whitney). Эта оценка неулучшаемая. 

//рогатая сфера александра
//мордель брот -- фрактальная геометрия природы

Таккенс дал ответ на вопрос, можем ли мы прогнозировать. Если мы от рассмотрения временного ряда перейдем к рассмотрению пространства z-векторов, то выбирая размерность z-векторов достаточно большой, мы можем добиться того, чтобы у нас наблюдалось взаимнооднозначное соответствие между истинной динамикой системы, которую мы не знаем, и динамикой в пространстве z-векторов, которая нам прекрасно известна. 

Пусть есть многообразие $M^n$, по которому протекает движение истинной системы, которую мы не знаем. 

_Замечание._ Вообще говоря странные аттракторы хаотических динамических систем не являются многообразиями даже локально. (ведь они геометрически сложные фрактальные объекты) Но мы можем считать что $M^n$ является минимальным иннерционным многообразием, проще говоря, минимальным многообразием, который объемлет странный аттрактор. Можем считать для простоты, что мы работаем именно с многообразием. 

Пусть есть так же отображение $h: M^n -> RR^d$, будем называть его наблюдаемой. Тогда теорема Таккенса формулируется следующим образом: Если:

1) многообразие $M^n$ и отображение $h$ являются как минимум дважды дифференцируемыми;

2) $h(M^n)$ является диффиоморфизмом, взаимно непрерывным и взаимно дифференцируемым отображением;

3) выполнены условия теоремы Уитни $d >= 2n + 1$, то есть с топологической точки зрения $h$ является вложением;

4) некоторые технические условия;

Тогда существует взаимнооднозначное отображение между динамикой истинной системой и динамикой в пространстве z-векторов. 

#pagebreak()

Пусть есть набор $d$-мерных точек. Пусть $tau -$ физическое время, которое протекает между нашими наблюдениями. Соответственно, в силу того, что $h$ является диффиоморфизмом, отображение $h^(-1)$ отоброзит точку этого ряда в точку на истинной неизвестной нам траектории. Обозначим координату $z_i$ как $x_i in M^d$. Рассмотрим сдвиг по траектории на время $tau$. Обозначим сдвиг как $g^tau$, тогда после сдвига мы попадем в точку $x_(t+1) = g^tau x_t.$ Давайте на точку $x_(i+1)$ подействуем отображением $h$, $x_(i+1) = q^tau x_i ->^h z_(i+1)$, то есть получаем сдвиг временного ряда на $tau$.

Опишем все сказанное математически. 

$ z_(i+1) = h(g^tau (h^(-1)(z_i))) $

Обозначим такое сквозное отображение как $Lambda(z_i)$. Согласно теореме Таккенса такое отображение является взаимнооднозначным и дифференцируемым. 

$Lambda: z_i -> z_(i + 1)$, хотим оставить от этого отображения только одну компоненту. 

$ y_(i+1) = zeta_n (y_i, y_(i - 1), dots, y_(i - d + 1)) $

Где $zeta_n$ тоже взаимнооднозначное дифференцируемое отображение.

Формально мы показали, что следующее наблюдение временного ряда является взаимнооднозначной дифференцируемой функцией $d$ его прошлых наблюдений.

Выходит, мы можем прогнозировать. Мы не знаем $zeta_n$, теорема Таккенса его не специфицирует, но абсолютно все методы прогнозирования занимаются тем, что строят некоторую его аппроксимацию, например нейронная сеть, универсальный адаптивный аппроксиматор. 

Давайте сделаем что-то простое, например, разложим $zeta_n$ в ряд Тейлора и обрубим все члены, кроме линейных. Мы получим $alpha_0 + alpha_1 y_i + dots + alpha_d y_(i - d +1) + epsilon$.

По сути получили классическую постановку линейного регрессионного анализа. Мы не знаем коэфициенты, но можем их найти, например, с помощью МНК, или метода максимального правдаподобия. Приближение очень грубое, чем сложнее приближение, тем более сложный прогноз. Мы можем по разному аппроксимировать функцию $Lambda$, но тот факт, что мы можем прогнозировать, дается нам теоремой Таккенса. 

Если мы применим это утверждение $k$ раз, то получим утверждение, что мы можем прогнозировать на $k$ шагов вперед.   
