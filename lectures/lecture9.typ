= Лекция 9. Качественная теория ОДУ. Нейродифференциальные уравнения

Нужно идею дифференциальных уравнений развить до тех идей, которые будут использоваться в сильном искусственном интеллекте.

Качественные и количественные теории ОДУ.

Количественная теория ОДУ предполагает, что базовым объектом исследования является само по себе ОДУ, и представляет собой некую сводку правил аналитического решения такого рода уравнения. Классический подход того, как излагать ОДУ. Этот подход сталкивается с двумя возражениями, когда речь идет о реальных, практических задачах, связанных с ОДУ.

Первое возражение: Подавляющее число ОДУ не имеет аналитического решения. (не мы не можем его найти, а его просто нет, доказан факт его отсутствия) И в этом случае мы выходим за рамки классических ОДУ и приходим в другую математическую дисциплину: численные методы или вычислительная математика. Это рассказ о том, как решать ОДУ с помощью компьютера. Термин носит название численно интегрировать ОДУ. 

_Какая базовая проблема ОДУ на примере примера с прошлой лекции? При решении у нас возник коэфициент рождаемости, что, на самом деле, сложно вычислимая вещь. Коэфициент рождаемости еще звучит реально, но когда мы уходим за рамки простых физических процессов, мы обнаруживаем, что эти коэфициенты определяются нестрого. Когда речь идет о химических-физических задачах, предметы подвержены каким-то простым законам и нам легко их описать, а если система сложная (а сильный интеллект относится к сложной системе), то законов тоже много и они сложны. _

В математической экологии есть классическое диференциальное уравнение хищник-жертва (модель Лотки-Вольтерры)

Если $x(t), y(t)$ это количество жертв и хищников, то выполняется следующее
$ accent(x, dot)(t) = alpha x(t) - gamma x(t) y(t) $
$ accent(y, dot)(t) = beta y(t) + gamma x(t) y(t) $

Так же в математической экологии есть десятки версий таких уравнений для разных фаун, однако они все улавливают фундоментальные, качественные принципы поведения системы. 
\ \ \
Отсюда следует следующее:

При исследовании реальных процессов, описываемых ОДУ, нам важен не вид конкретного дифференциального уравнения, а качественные свойства, которые эту модель описывают. Глобально эта идея носит название мягкого моделирования. А применительно к ОДУ, она дает нам качественную теорию ОДУ. Качественная теория работает с качественными свойствами и базовым ее объектом являются так называемые потоки ОДУ, о которых мы поговорим позже.

Если мы хотим работать со сложными системами, то мы должны работать, во-первых, с численными методами, во-вторых, с качественной теорией ОДУ. Это наша базовая интенция.

_Качественная теория: не останавливаемся на решении одного уравнения, нам интересно изучать свойства набора уравнений_
// Мир нелинеен означает что такие дифференциальные уравнения не имеют линейных зависимостей 

Численные методы решения ОДУ. В подавляющем большинстве аналитических решений не существует и нужно их искать численно. Здесь возможны два варианта: Задача Коши :

$ cases((d y)/(d x) = f(x, y) : y in RR^1,
y(0) = y_0
) $

И краевая задача. Рассмотрим, как решать задачу Коши в одномерном случае, но в общем случае алгоритм такой же, просто более сложный, матричный. 

Разобьем область определения $y in [0, x_max]$ на маленькие кусочки длины $h$. Стоит подчеркнуть, что $h$ являются малыми, но не бесконечно малыми, около $10^(-2), 10^(-3)$ в сравнении с $x_max$. Рассмотрим первый из таких промежутков, лежащий от $0$ до $h$.  Перепишем наш диффур в виде $d y = f(x, y) d x$. Сначала применим оператор интегрирования $integral_0^h$. Применим к нашему диффуру формулу Ньютона-Лейбница на этом участке. 

$ y(h) - y(0) = integral_0^h f(x,y) d x $
$ y(h) = y(0) + integral_0^h f(x,y) d x $

Если нам удастся оценить этот интеграл, то мы сможем найти $y(h)$. 

#pagebreak()

Самый простейший метод оценки -- посчитать площадь трапеции под функцией. 

#image("../images/lecture_9_images/picture1.jpeg", width: 25%)

Давайте рассмотрим площадь прямоугольника, она будет примерно равна $ h dot f(0, y(0)) $

Это выражение мы можем вычислить, значит мы знаем $y(h)$. Давайте рассмотрим следующий кусочек $[h, 2h]$. Такой же логикой мы можем найти $y(2h),$ зная $y(h).$ В результате последовательного применения численного интегрирования мы получаем последовательность значений исходной функции, удовлетворяющей граничным условиям и дифференциальному уравнению. Находим решение для этой функции в конечном множестве точек. Так как $h$ по сути параметр, мы можем сделать эти значения сколь угодно частыми. Если мы нарисуем график, то увидим, что аналитическое и численное решения совпадают. Таким образом, нет большого смысла искать аналитическое решение в реальных задачах. 

Следует иметь в виду, что любой численный метод обладает погрешностью, вносимой на каждом шаге численного интегрирования. Чем дальше мы двигаемся, тем большую погрешность мы внесли в решение. Для рассмотренного метода Эйлера погрешность на каждом шаге составляет $O(h).$ Значит в самом неудачном случае двигаясь по промежутку мы можем внести погрешность сравнимую с значением функции. 

К счастью, существуют гораздо более эффективные методы численного интегрирования. Самым базовым является метод Рунге-Кутты (4-го порядка). Он тоже основывается на достаточно логичных принципах, мы функцию разности между истинной и аппроксимированной функцией расскладываем в функцию Тейлора. 

Погрешность, которую дает этот метод на каждом шаге: $O(h^s),$ где $s -$ взятый порядок. 
// коэффициенты в диффуре будут менее точными, чем этот метод
// ДЗ: посмотреть вывод метода Рунге-Кутты
#pagebreak()
Отступление. Нейродифференцальные уравнения.
// Какое-то время назад в ии была революция, когда все перешли с малых моделей на глубокое обучение, далее будет революция, заключающаяся в использовании данных диффуров

Какое-то время назад в нейронных сетях произошла революция, когда все перешли с малых моделей на глубокое обучение. Следующей же революция будет связана с нейродифференцальными уравнениями.

У нас некая нейронная сеть, например Resnet, $x_(n + 1) in RR^s = x_n + f(x_n)$ -- состояние следующего слоя. Состояние следующего слоя -- функция активации от текущего слоя. В теории нейродифференцальных уравнений мы говорим: давайте считать, что процесс протекания информации от входа в нейронную сеть к выходу -- процесс, разворачивающийся в дискретном времени с шагом единица.

Для первого слоя мы находимся в $n = 0, n = 1, dots, n = N, Delta t = 1$

Это означает, что у нас есть две величины: одна $N -$ число слоев, а вторая, $T -$ (физическое) время, которое протекло от момента подачи информации до момента выхода. 

До этого мы их не различали, потому что считали, что $T = N dot underbrace(Delta t, =1)$. Дальнейшая логика изложения: что такое переход от неглубокого обучения к глубокому? Ранее $N$ было небольшим, позже мы обсудим почему, но произошла некая революция, модели глубокого обучения, где мы сказали, что $N -$ весьма большая величина. Нейродиффурщики сказали: давайте доведем процесс до конца. Давайте не считать время $T$ фиксированным, а $N$ устремим к бесконечности. $T - $ константа, $N -> infinity,$ следовательно, $Delta t -> 0$. Перепишем состояния слоев:

$ (x(t + Delta t) - x(t))/(Delta t) = f(x(t)) $

Давайте применим оператор предельного перехода к обоим частям равенства:

$ lim_(Delta t -> 0) (x(t + Delta t) - x(t))/(Delta t) = f(x(t)) $

Получаем ОДУ: $(d x)/(d t) = f(x(t))$. Получаем обычную задачу Коши, остается только добавить, что $x(0) = x_0$. Следуя традиции классического машинного обучения мы смотрим не на истинный объект (нейродиффур), а какое-то приближение: $ x_(n + 1) = x_n + f(x_n) $

// прием в математике: В полночь пойти на кладбище александро-невской лавры и на могиле Эйлера пить алкогольные напитки

Если внимательно посмотреть на формулу Эйлера, то мы обнаружим, что на самом деле мы для исследования истинного объекта используем старый метод Эйлера. Сейчас идет стремительный переход к нейродифференцальным уравнениям, мы поговорим о них далее. 

То, что мы сейчас расписали -- это лишь то, что называется прямой ход нейронной сети. Мы написали уравнение, которое описывает изменение состояния. Здесь у нас слоев бесконечно количество, но сути это не меняет. Понятно, что здесь мы не рассмотрели главного: в нашей нейронной сети нет весов. При рассмотрении нейродифференцальных уравнений мы, конечно же, должны рассматривать и вектор-функцию весов $w(t)$, где $w(t)$ есть некий аналог матрицы (тензора) весов нейронной сети, который получается из него тем же предельным переходом $Delta t -> 0$. Тогда мы имеем, что наш диффур зависит не только от состояния $x(t),$ но и от $w(t), $ которая и подлежит определению.

$ (d x)/(d t) = f(x(t), w(t)) $

Мы будем определять ее следующим образом: 

У нас есть некая функция ошибки (потери), которая характеризует состояние сети на выходном слое, или в наших новых терминах состояние сети в последний момент времени $T$ и некие указания учителя $x^*$, причем эту функцию мы стремимся минимизировать.

$ L(x(T), x^*) -> min $

Например квадратичная функция ошибки:

$ L = 1/2 [x(T) - x^*]^2 $
$ d x = f(x(t), w(t)) d t $
$ integral_0^T d x = integral_0^T f(x(t), w(t)) d t $
$ x(T) = underbrace(x(0), = x_0) + integral_0^T f(x(t), w(t)) d t => $
$ => L = 1/2 [ x_0 + integral_0^T f(x(t), w(t)) d t - x^*]^2 -> min $

$w(t) -$ наша степень свободы. Наша задача найти такое $w(t), $ чтобы наша функция ошибки была минимальной. 

Хотим найти минимум этого функционала при ограничениях:

$ cases(
  (d x)/(d t) = f(x(t), w(t)),
  x(0) = x_0
) $

Это делается вариационными вычислениями, тема следующей лекции.

