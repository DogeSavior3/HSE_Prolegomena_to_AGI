= Лекция 18. 

Напомним, что базовой задачей является не просто научиться прогнозировать на один или несколько шагов вперед, но научиться прогнозировать на много шагов вперед. Собственно говоря, на число шагов, сопоставимое или превышающее горизонт прогнозирования. На один шаг могут прогнозировать практически любые модели, но на много шагов вперед ни одна из них прогнозировать не может. 

Введение шаблонов прогнозирований уже позволяет прогнозировать на несколько шагов вперед для того,  чтобы как-то сдвинуться с этой мертвой точки экспоненциальной ошибки.

Разумеется, это является неким полиотитом, и мы в любом случае должны использовать стандартную стратегию при прогнозировании на много шагов вперед, а именно стратегию прогнозирования по уже спрогнозированным точкам. Если мы закончили наблюдать в ряд на какой-то точке и получили прогноз для следующей точки, то это прогнозное значение мы можем считать истинным и использовать, прогнозировать уже с учетом этого значения. Тем самым мы, теоретически, можем двигать наш прогноз до бесконечности. 

Когда мы говорим "стандартную стратегию", надо ясно понимать, что существует очень много разных подходов организовать прогнозирования, и такой наивный подход "точка-за-точкой" не самый лучший и не самый эффективный. Но как бы там ни было никакая из стратегий и никакой из методов прогнозирования не решает нашей базовой проблемы -- экспоненциального роста ошибки с числом шагов вперед, на которое мы хотим спрогнозировать. Значит нужно придумать что-то еще. Прорывной оказалась следующая идея:

Идея непрогнозируемых точек. Использование шаблонов в прогнозировании позволяет получить для каждой точки, для которой мы хотим отыскать прогноз, некоторое, обычно весьма значительное число возможных прогнозных значений. Мы ввели понятие $hat(S)_(t + k),$ как множество возможных прогнозных значений для точки, которая стоит на $k$ позиций вперед во временном ряду от точки конца наблюдений $t$. Раз у нас есть ножество возможных прогнозных значений, то мы можем задасться вопросом, можно ли получить некий возможный прогноз, исходя из этого ножество возможных прогнозных значений. 

Если у вас есть тысяча возможных прогнозных значений, где, условно, 500 значений лежит компактно у $+1$, а еще 500 у $-1$, то стандартные методы вроде "усреднить" ведет к ....

Другая ситуация, когда ножество возможных прогнозных значений размазано вдоль ..... и никакой кластерной структуры выделить нельзя.

Хорошая ситуация, когда все точки сконцентрированы вокруг какого-то значения, и, соответственно, мы можем получить некий разумный алгоритм получения единого прогнозного значения. Эта идея привела к тому, что мы для ножество возможных прогнозных значений вычисляем две ..... функции:

Функцию $xi(hat(S)_(t + k)) : cases(
  1", если точка прогнозируемая",
  0", иначе"
)$

Вторая функция -- единое прогнозное значение $hat(y)_(t + k) = hat(y)(hat(S)_(t + k))$. 

Такой подход мгновенно превращает классическую постановку задач прогнозирования как задачи однокритериальной оптимизации (один критерий качества) в задачу неклассическую постановку, двухкритериальную оптимизацию.

Первый критерий -- хотим минимизировать ошибку на прогнозируемых точках: $ I_1 = sum_(t in T) xi(hat(S)_(t + k)) M [y_(t + k) - hat(y)_(t + k)]^2 -> min $

И второй критерий: хотим минимизировать количество непрогнозируемых точек:

$ I_2 = sum_(t in T) [1 - xi(hat(S)_(t + k))] -> min $

Надо ясно понимать, что переход к двухкритериальной задаче оптимизации -- всегда некоторое усложнение, тем более даже не на алгоритмическом уровне построения эффективного метода решения, сколько на уровне концептуальном. Очевидно, что существует решения, которые будут хуже по первому критерию и лучше по второму, или наоборот. 

В теории многокритериальной оптимизации вводится множество Паретта, так называемое множество неухудшаемых решений. Выход из такого множества приводит либо к .. 

Так же фронт Паретта. Решения, которые выходят на фронт Паретта, это те решения, которые являются несравнимыми между собой. Если мы возьмем любые два произвольных решения, из множества Паретта, то одно из них будет лучше по первому критерию, другое по второму. Это некоторая концептуальная сложность, поскольку не очень понятно, какое решение в итоге выбирать. 

//отступление про многокритериальную оптимизацию

//.......

Важно сказать, что такая постановка задачи привела к возможности решения базовой постановки, а именно нам действительно удаестся прогнозировать за горизонтом прогнозирования. Метафорой можно сказать, что мы пересекаем болота по кочкам. Мы прыгаем от одной прогнозируемой точки к другой, игнорируя непрогнозирования точки.

Таким образом нам удалось заскочить далеко за горизонт прогнозирования, в то время как классические алгоритмы тонут в непрогнозируемых точках. Так мы сформулировали задачу как задачу двухкритериальной оптимизации. Но это лишь формулировка задачи. По сути выдвинута лишь гипотеза, а как ее проверить?

Давайте предположим, что у нас есть некий идеальный алгоритм определения непрогнозирования точек, идеальная $xi$. Для тестовой выборки выглядит она достаточно просто. Алгоритм просто обращается к истинному значению и сравнивает с прогнозируемым. Если разница $|y_(t + k) - hat(y)_(t + k)|$ меньше некого $epsilon$, то функция $1$. Такой алгоритм назвали Демоном......

Так мы получаем идеальную функцию $xi$, но идеальной функции $hat(y)$ у нас все еще нет. 

Для первого критерия Нарисуем такой график. По оси абсцисс будет идти $k$, количество прогнозируемых точек, а по оси .... будет процент непрогнозируемых точек. 

Для второго критерия отложим среднюю ошибку. Для классического алгоритма мы получаем экспоненциальную ошибку, а для нашего на прогнозируемых точках ошибка перестала расти экспоненциально, она ведет себя как функция порядка $k$.

Это означает, что задача решена. ..... Мы побили закон Ляпунова, но ценой того, что даем прогнозирование не во всех точках. Для многих задач это подходит, например, торговли на бирже, ведь нам необязательно торговать в каждый момент времени. Но, например, в задаче прогнозирования инфаркта, он может произойти, теоретически, в непрогнозируемой точке. 

// отступление

Мы не сказали главную загвоздку -- вычисление функции $hat(xi)$. Решение такой задачи называется аппроксимацией демона. Так же не совсем понятно как искать $hat(y)$. Множество вариантов можно посмотреть в статье Gromov Baranov 2024.

А пока приведем несколько примеров таких функций:

Пусть у нас есть $hat(S)_(t + k)$. И мы можем их скластеризировать. У нас выделится какое-то количество кластеров, один из которых будет наибольшим. Мы говорим, что если размер максимального кластера больше некоторого порога $sigma: $ $max_i |c_i| >= sigma(90%)$, то достаточно очевидно, что мы можем считать такую точку прогнозированной. Тогда выберем $hat(y)$ как центройд кластера. 

Другой подход. Давайте считать множество возможных прогнозных значений не для одной позиции $t +k$, а для нескольких позиций сразу: $hat(S)_(t + k), hat(S)_(t + k + 1), hat(S)_(t + k + 2), dots$

Если ошибка растет на первых трех шагах, то эту точку уже можно считать непрогнозируемой. Получилось, что он уже не функционирует. 

Еще один подход: давайте брать в рассмотрение не только максимальный кластер, но и, например, второй по размеру кластер. Тогда вместо единого прогнозного значения мы получаем два единых прогнозных значений, из которых стартуют независимые траектории. Если оказывается, что в какой-то момент времени эти траектории пересекаются (достаточно близко сходятся), то речь идет о прогнозируемой точке. В качестве прогнозируемого значения выбирается среднее. При этом необязательно длить все эти траектории, ведь их будет очень много. 

...............

Какие здесь есть открытые вопросы? У нас есть множество Паретта, 

...............

//отступление про странный атрактор
